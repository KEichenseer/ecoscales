---
title: "Space-time plots"
author: "Lyndon Estes"
date: "December 3, 2015"
output: 
  html_document:
    toc: yes
    number_sections: true
    toc_depth: 3
---

# Space versus time analysis
```{r, message = FALSE, warning=FALSE, results='hold'}
library(ecoscales)
library(readxl)
# library(stringdist)
library(maptools)
library(RColorBrewer)
library(viridis)
# library(zyp)

p_root <- set_base_path("ecoscales")
p_dat <- fp(p_root, "external/data/result")
p_calib <- fp(p_root, "external/data/calibration")
p_fig <- fp(p_root, "paper/figures")
p_til <- fp(p_root, "external/data/misc")

# dat <- as.data.table(read_excel(full_path(p_dat, "merged.xlsx")))
dat <- as.data.table(read_excel(full_path(p_dat, "mergedffffff.xlsx")))
# dato <- as.data.table(read_excel(full_path(p_dat, "archive/mergedfff.xlsx")))
cal <- data.table(read_excel(fp(p_calib, "merged_calibrationffffff.xlsx")))
full <- data.table(read_excel(fp(p_dat, "full_no_treuer_toss.xlsx")))
# full <- data.table(read_excel(fp(p_dat, "full_less_Treuer.xlsx")))

# setnames(dato, "DOI/title", "DOI")
# dato[, DOI := gsub("DOI:|doi:|DOI: |doi: |DOI ", "", DOI)]
# dato[, DOI := gsub("\\s", "", DOI)]  # remove DOI and whitespace

# clean up DOIs
setnames(dat, "DOI/title", "DOI")
dat[, DOI := gsub("DOI:|doi:|DOI: |doi: |DOI ", "", DOI)]
dat[, DOI := gsub("\\s", "", DOI)]  # remove DOI and whitespace
cal[, DOI := gsub("\\s", "", DOI)]  # remove DOI and whitespace
full <- gsub("DOI:|doi:|DOI: |doi: |DOI ", "", full$DOI)
full <- gsub("\\s", "", full)  # remove DOI and whitespace

# subset main results and define study type
dat <- dat[, names(dat)[1:22], with = FALSE]
dat[, study_type := tolower(study_type)]
# kwords <- c("field", "paleo", "remote", "automated", "other")
# tst <- dat[, study_type]

kwords <- cbind(c("field", "paleo", "remote", "automated", "other"), 
                c("field", "paleo", "remote", "automated", "other"))
# for(i in 1:length(kwords)) dat[like(study_type, kwords[i]), st := i]
for(i in 1:nrow(kwords)) {
  dat[like(study_type, kwords[i, 1]), study_type := kwords[i, 2]]
}
# cbind(tst, dat[, study_type])

kwords <- cbind(c("field", "direct", "paleo", "remote", "automated", "other"),
                c("field", "field", "paleo", "remote", "automated", "other"))
cal[, study_type := tolower(study_type)]
# tst <- cal[, study_type]
for(i in 1:nrow(kwords)) {
  cal[like(study_type, kwords[i, 1]), study_type := kwords[i, 2]]
}

```

## Data prep
### Merge full and calibration datasets
```{r, message=FALSE, warning=FALSE, results='hide'}
dat2 <- copy(dat[, .(DOI, study_type, plot_res, n_sites, act_ext, eff_ext,
                     samp_dur, t_btwn_samp, act_dur, eff_dur)])
# selind <- 3:ncol(dat2)
selind <- 4:ncol(dat2)
for(j in names(dat2)[selind]) set(dat2, j=j, value = as.numeric(dat2[[j]]))
rnms <- c("DOI", "study_type", "feature", "observer", "plot_res",
          "n_sites", "act_ext", "eff_ext", "samp_dur", "t_btwn_samp",
          "act_dur", "eff_dur")
calr <- cal[, rnms, with = FALSE]
selind <- c(3, 5:length(rnms))
for(j in names(calr)[selind]) set(calr, j=j, value = as.numeric(calr[[j]]))

# str(calr)
# select down to columns of interest
# st <- calr$study_type[which(calr$feature != 99)]
calr_type <- calr[feature != 99, { 
  list(unique(study_type[!is.na(study_type)]))
}, by = .(DOI, feature)][, list("study_type" = unique(V1)), by = DOI]

# calr[DOI == "10.1007/s00442-005-0338-3" & feature == 2, 
#      mean(eff_dur, na.rm = T)]
calr <- calr[feature != 99, {
  lapply(.SD, function(x) mean(x, na.rm = TRUE))
},  by = .(DOI, feature), .SDcols = rnms[c(5:ncol(calr))]]
length(unique(calr$DOI)) + length(unique(dat$DOI))  # 134 papers providing dat

# combine full and calibration
# calr[, samp_dur := NA]
calr[, samp_dur := NA]
calr[, "feature" := NULL] #c("DOI", "feature") := NULL]
calr <- merge(calr, calr_type, by = "DOI")
setcolorder(calr, names(dat2))
datf <- rbind(cbind("dsrc" = 1, dat2), cbind("dsrc" = 2, calr))

# drop one calibration study only done by one observer
datf <- datf[DOI != "10.1016/j.agee.2013.11.021", ]

# DOIs of additional studies pointed to as data sources
doisup <- c(unique(cal$DOI_data_source), unique(dat$DOI_data_source))
doisup <- doisup[!is.na(doisup) & doisup != "NA" & doisup != "-"]
doisup <- unlist(strsplit(doisup, ";"))

```

`r length(unique(datf$DOI))` papers  main analysis

`r length(unique(full)) + (57 - 19)` papers main analysis (adding in Tim's rejected number separately)

`r (length(unique(full)) + (57 - 19)) / 42918 * 100` % of all papers since 2004

`r length(unique(doisup))` additional papers or other publications tracked down

`r nrow(datf)` records (ecological observations)

<a href="#top">Back to top</a>

### Adjustments/fixes
1. One could argue that if `samp_dur != act_dur` and `t_btwn_samp == 0`, set `act_dur == samp_dur`, but in a small number of cases there is a good reason to have to `act_dur != samp_dur` (e.g. DOI 10.1890/08-0611.1 when farmers were asked about their seasonal management practices--time period of interview was clearly != to information being collected))
2. Prior to scaling the data in log10 space, we need to make one adjustment. Time between samples has many 0 values because many observations are simply one-offs.  Set these to an arbitrarily large value which will represent clear separation on axis from high frequency studies. A function is defined here for this purpose, and will be applied after bootstrapping is done. 

```{r, message=FALSE, warning=FALSE, results='hide'}
# apply fixes (mostly to account for observer omissions)
# 1. if samp_dur != act_dur and t_btwn_samp == 0, set 
#    act_dur == samp_dur
names(datf)
datf[t_btwn_samp == 0 & (act_dur != samp_dur), ]  
dat[t_btwn_samp == 0 & (act_dur != samp_dur), c(1, 3:4, 9:15), with = FALSE]  
# function to set t_btwn_samp to 365 * 100000 to indicate once-off studies 
# when t_btwn_samp == 0.  
dat[act_dur < 1 & t_btwn_samp > 1000, ]
dat[act_dur > 1 & t_btwn_samp == 0, c(1, 3:4, 9:15), with = FALSE]
```

<a href="#top">Back to top</a>

## Analyses
### Resampling with uncertainties

Defined by the per variable CV based on uncertainty between observers. Here we are just going to use apply the variability to each variable for every observation (previous incarnation used uncertainty on just the variables listed as uncertain, with recalculation of dependent variables (`act_dur`, `act_ext`) made after pertubation [see commit prior to second or third commit on 4 April for code]).

```{r, eval=FALSE}
p <- c(0.58, 0.72, 0.41, 1.05, 1.26, 0.64)  # from calibration.Rmd

# Functions for bootstrapping with uncertainties
# sets unreplicated observations to arbitrarily large number
to_infin_byond <- function(tbtwn) ifelse(tbtwn == 0, 365 * 10000, tbtwn)

# randomly reorders two variables, for reassignment in case of physically 
# unrealistic values
rvord <- function(a, b) {
  v <- c(a, b)
  v <- v[sample(1:2)]
  v
}

# function for parse-eval in data.table i
psr <- function(a) eval(parse(text = a))

# bootstrapping function
resamp_func <- function(dat, p, iter) {
  vars <- c("plot_res", "act_ext", "eff_ext", "t_btwn_samp", "act_dur", "eff_dur")
  bout <- lapply(1:iter, function(x) {  # x <- 1
    if((x / 100) %in% 1:100) print(x)
    if(length(p) < length(var)) stop("p must be length of var", call. = FALSE)
    
    # dnew <- copy(datf[, .(plot_res, act_ext, eff_ext, t_btwn_samp, act_dur)])
    # dnew <- copy(datf[, vars, with = FALSE])
    dnew <- copy(dat[, vars, with = FALSE])
    # modifiers
    modvec <- sapply(p, function(y) {
      mv <- runif(nrow(dnew), min = 1 - y, max = 1 + y)
      mv[mv < 0] <- 0.00000001  # can't have reduction of more than 100%
      mv
    })

    # perturb and set any values < 0 to very small amount
    for(j in 1:ncol(modvec)) set(dnew, j = j, value = dnew[[j]] * modvec[, j])

    # correct physically impossible results 
    dnew[, plot_res := plot_res / 10000]  # set plot_res to ha
    dnew[plot_res > act_ext, plot_res := act_ext]  # plot_res can't be > act_ext
    dnew[plot_res > eff_ext, plot_res := eff_ext]  # or eff_ext
    dnew[act_ext > eff_ext, act_ext := eff_ext] # act_ext can't be > eff_ext
    dnew[t_btwn_samp > eff_dur, t_btwn_samp := eff_dur]  # int can't be > eff_dur
    dnew[act_dur > eff_dur, act_dur := eff_dur]  # act_dur can't be > eff_dur
    dnew[, plot_res := plot_res * 10000]  # set plot_res back to m2
    
    # set unreplicated to high value for first figure 
    dnew[, t_btwn_samp := to_infin_byond(t_btwn_samp)]  # set high
    dnew
  })
  return(bout)
}

# Run bootstrap
set.seed(1)
bootperturb <- resamp_func(datf, p = p, iter = 1000) 
```

```{r, echo = FALSE, eval=FALSE}
# check
par(mfrow = c(2, 3))
for(i in sample(1:1000, 6)) {
  plot(datf[, eff_ext], datf[, eff_ext] - bootperturb[[i]][, eff_ext], 
       pch = 20)
}
dev.off()
```

<a href="#top">Back to top</a>

## Plots
### Scaling in log10 space
```{r, eval = FALSE}
i <- 0.0000001
j <- rep(0, 16)
for(k in 1:length(j)) {
  i <- i * 10
  j[k] <- i
}

# temporal scales
tdt <- data.table("scaleval" = j, "time" = j)
tdt[, tlog := log10(time)]

# labels for temporal axes
# return interval
tlab1 <- c("second", "minute", "hour", "day", "week", "month", "year", "decade",
           "century", "millenium", "unreplicated")
taxis1 <- cbind.data.frame( 
  c(1 / (24 * 60 * 60), 1 / (24 * 60), 1 / 24, 1, 7, 30, 365, 365 * 10, 
    365 * 100, 365 * 1000, 365 * 10000))
taxis1 <- data.table(taxis1)
setnames(taxis1, names(taxis1), "days")
taxis1[, logdays := log10(days)]

tlab2 <- c("second", "minute", "hour", "day", "week", 
           "month","year","decade", "century", "millennium", "10 KA")  
taxis2 <- cbind.data.frame(c(1 / (24 * 60 * 60), 1 / (24 * 60), 1 / 24, 1, 7, 
                             30, 365, 365 * 10, 365 * 100, 365 * 1000, 
                             365 * 10000))
taxis2 <- data.table(taxis2)
setnames(taxis2, names(taxis2), c("days"))
taxis2[, logdays := log10(days)]

# plot resolution
alab1 <- c(expression(paste(NULL<="0.01 cm"^2)), expression("0.1 cm"^2), 
           expression("1 cm"^2), expression("10 cm"^2),
           expression("0.01 m"^2), expression("0.1 m"^2), 
           expression("1 m"^2), expression("10 m"^2),
           expression("100 m"^2), expression("1000 m"^2), 
           "1 ha", "10 ha", "100 ha", "1000 ha", 
           expression(paste(NULL>="10000 ha")))

aaxis1 <- cbind.data.frame(c(0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1,
                             1^2, 10, 10^2, 10^3, 10^4, 10^5, 10^6, 10^7, 10^8))
aaxis1 <- data.table(aaxis1)
setnames(aaxis1, names(aaxis1), "res")
aaxis1[, logres := log10(res)]

# effective extent
alab2 <- c(expression(paste(NULL<="0.01 m"^2)), expression(paste("0.1 m"^2)), 
           expression("1 m"^2), expression("10 m"^2), 
           expression("100 m"^2), expression("1000 m"^2), expression("1 ha"),
           expression("10 ha"), expression("100 ha"), expression("1000 ha"), 
           expression(paste(10^4 , " ha")), expression(paste(10^5 , " ha")), 
           expression(paste(10^6 , " ha")), expression(paste(10^7 , " ha")), 
           expression(paste(10^8 , " ha")), expression(paste(10^9 , " ha")), 
           expression(paste(10^10 , " ha")))

aaxis2 <- cbind.data.frame(
  c(0.01 / 10000, 0.1 / 10000, 1 / 10000, 10 / 10000, 100 / 10000, 1000 / 10000,
    1, 10, 10^2, 10^3, 10^4, 10^5, 10^6, 10^7, 10^8, 10^9, 10^10))
aaxis2 <- data.table(aaxis2)
setnames(aaxis2, names(aaxis2), "area")
aaxis2[, logarea := log10(area)]
```
```{r, eval = FALSE, echo = FALSE}
save(aaxis1, aaxis2, taxis1, taxis2, alab1, alab2, tlab1, tlab2, 
     file = fp(p_dat, "derived/dimbreaks.rda"))
```

<a href="#top">Back to top</a>

### Figure 1 
Histograms for each of the four main dimensions.
```{r, eval=FALSE}
# length(which(datf$study_type %in% c("remote", "other"))) / nrow(datf)
datf[t_btwn_samp == 0, .N] / datf[, .N]  # 37% of samples are once-offs
datf[act_dur == samp_dur, .N] / nrow(datf)
datf[(act_dur == samp_dur) & t_btwn_samp != 0]
datf[(act_dur != samp_dur) & t_btwn_samp == 0]

hdat <- cbind("res" = log10(datf$plot_res), "aext" = log10(datf$act_ext),
              "ext" = log10(datf$eff_ext), 
              "int" = log10(to_infin_byond(datf$t_btwn_samp)), 
              "adur" = log10(datf$act_dur), "dur" = log10(datf$eff_dur))
# NA data checks
# hdat[is.na(rowSums(hdat)), ]
# dat[DOI %in% datf[which(is.na(rowSums(hdat))), DOI], ]
# dat[DOI %in% datf[which(is.na(hdat[, 6])), DOI]]
# dat[DOI %in% datf[which(is.na(hdat[, 3])), DOI]]
# calr[DOI %in% datf[which(is.na(hdat[, 3])), DOI] & is.na(eff_ext)]

hdat <- data.table(hdat)

# set bounds for histograms
# first set variable limits
vlr <- range(aaxis1$logres)  # for resolution
vle <- range(aaxis2$logarea)  # for extent

# then apply limits to main (unperturbed) set of variables
hdat[res < vlr[1], res := vlr[1]]  # set min res to 0.01 cm"^2 
hdat[res > vlr[2], res := vlr[2]]  # set max res to 10000 ha 
hdat[ext < vle[1], ext := vle[1]]  # set minimum extent -6 (0.01 m2) 
hdat[ext > vle[2], ext := vle[2]]  # set maximum extent 10 (10^10 ha)
# hdat[aext < vle[1], aext := vle[1]]  # set minimum extent -6 (0.01 m2) 
# hdat[aext > vle[2], aext := vle[2]]  # set maximum extent 10 (10^10 ha)


# histograms from bootstrap
brksl <- list(aaxis1$logres, aaxis2$logarea, taxis1$logdays, taxis2$logdays)  

# Assign bootstrapped values to bins, making necessary adjustments as needed 
vs <- c("res", "ext", "int", "dur")
hdat_bs <- lapply(1:length(bootperturb), function(x) {  # x <- 1
  print(x)
  DT <- bootperturb[[x]]
  hdb <- cbind("res" = log10(DT$plot_res), "ext" = log10(DT$eff_ext), 
               "int" = log10(to_infin_byond(DT$t_btwn_samp)), 
               "dur" = log10(DT$eff_dur))
  hdb <- data.table(hdb)
  hdb[res < vlr[1], res := vlr[1]]  # set min res to 0.01 cm"^2 
  hdb[res > vlr[2], res := vlr[2]]  # set max res to 10000 ha 
  # hdb[, lapply(.SD, range, na.rm = TRUE)]
  hdb[ext < vle[1], ext := vle[1]]  # set minimum extent -6 (0.01 m2)
  hdb[ext > vle[2], ext := vle[2]]  # set maximum extent 10 (10^10 ha)
  hists <- lapply(1:length(vs), function(y) {  # y <- 2
    #print(y)
    hdbv <- hdb[[vs[y]]]
    hbrks <- brksl[[y]]
    hdbv[hdbv < hbrks[1]] <- hbrks[1]
    hdbv[hdbv > hbrks[length(hbrks)]] <- hbrks[length(hbrks)]
    h <- hist(hdbv, breaks = hbrks, plot = FALSE)
    h$density <- h$counts / sum(h$counts) * 100
    h
  })
})

# convert density values for each dimension to data.tables
hdat_bsdt <- lapply(1:length(vs), function(x) { # x <- 1
  mat <- do.call(rbind, lapply(1:length(hdat_bs), function(y) {
    hdat_bs[[y]][[x]]$density
  }))
  DT <- data.table(mat)
})

### Bootstrapped histograms (now main figure)
qtf <- function(x) c(mean(x), quantile(x, probs = c(0.025, 0.975))) # quant f
xlabs <- c("Resolution", "Extent", "Interval", "Duration")
# axes <- list("aax1" = aaxis1$logres, "aax2" = c(-6, aaxis2$logarea), 
#              "tax1" = taxis1$logdays, "tax2" = taxis2$logdays)
axes2 <- list("aax1" = aaxis1$logres, "aax2" = aaxis2$logarea, 
              "tax1" = c(taxis2$logdays[-c(10:11)],
                         mean(taxis2$logdays[c(10:11)])), 
              "tax2" = taxis2$logdays)
axlabs <- list(alab1, alab2, tlab1[-10], tlab2)
mga <- -0.34
sfigl <- -1.2
cxa <- 0.8
cxl <- 0.9
reds <- brewer.pal(9, name = "Reds")
blues <- brewer.pal(9, name = "Blues")
# plot(1:2, pch = 20, col = blues[c(4, 6)])
yl <- 35
cols <- c(reds[c(4, 6)], blues[c(4, 6)])

# calculate mean, 2.5th, 97.5th
hdatstat_bt <- lapply(1:length(hdat_bsdt), function(x) {
  h <- hdat_bsdt[[x]][, lapply(.SD, qtf)]
  hmu <- unlist(h[1, ])
  h2 <- unlist(h[2, ])
  h98 <- unlist(h[3, ])
  list("mu" = hmu, "p2" = h2, "p98" = h98)
})
names(hdatstat_bt) <- vs

# pdf("paper/figures/hists_bs.pdf", width = 5, height = 5)
png("paper/figures/hists_bs.png", width = 5, height = 5, res = 600, 
    units = "in")
par(mfrow = c(2, 2), mar = c(4.5, 1, 1, 1), oma = c(2, 3, 0, 0))

for(i in 1:length(hdatstat_bt)) {  # i <- 1
  hvals <- hdatstat_bt[[i]]
  axv <- brksl[[i]]
  mids <- sapply(2:length(axv), function(x) mean(c(axv[x], axv[x - 1])))

    # base plot
  yl <- ifelse(i %in% c(1, 2), 25, 40) 
  plot(c(axv[1], axv[length(axv)]), c(0, yl), ylab = "", las = 2, 
         xaxt = "n", xlab = xlabs[i], ylim = c(0, yl), main = "", 
         col = cols[i], cex.lab = cxl, pch = "", bty = "l", axes = FALSE)
  
  # create polygons to mimic histograms of varying width
  sapply(2:length(axv), function(x) {  # x <- 2
    px <- axv[c(x - 1, x)]
    pxs <- c(rep(px[1], 2), rep(px[2], 2), px[1])
    pys <- c(0, rep(hvals$mu[[x - 1]], 2), 0, 0)
    polygon(pxs, pys, col = cols[i])
  })
  axis(1, at = axes2[[i]], labels = axlabs[[i]], las = 2, 
       cex.axis = cxa, tcl = -0.2, mgp = c(2, 0.25, mga))
  axis(2, las = 2, cex.axis = cxa, tcl = -0.2, mgp = c(2, 0.25, mga))
  if(i %in% c(1, 3)) {
    mtext("Percent of observations", side = 2, line = 1.5, cex = 0.8)
  }
  mtext(LETTERS[i], side = 3, line = sfigl, cex = 0.8, adj = 0.05)

  # confidence intervals
  points(mids, hvals$p2, pch = "-", cex = 1, col = "grey40")
  points(mids, hvals$p98, pch = "-", cex = 1, col = "grey40")

}
dev.off()

# stats
# bootp_dt
# axisl <- list(aaxis1$logres, c(-6, aaxis2$logarea), taxis1$logdays,
#               taxis2$logdays)
# hdatstat <- lapply(1:4, function(x) {
#   h <- hist(hdat[[x]], breaks = brksl[[x]], plot = FALSE)
#   h$density <- h$counts / sum(h$counts) * 100
#   h
# })
```
```{r, echo = FALSE, eval = FALSE}
save(hdat, hdatstat_bt, file = fp(p_dat, "derived/hdatstat.rda"))
```

Statistics associated with Figure 1: percentage of observations falling within different scale ranges, and associated with different observational methods
```{r, echo = FALSE}
load(fp(p_dat, "derived/hdatstat.rda"))
load(fp(p_dat, "derived/dimbreaks.rda"))
```
```{r}
# res
hdatst <- hdatstat_bt$res$mu
l <- length(hdatst)
hds <- c("10cm^2 - 1 m^2", "<1m^2", "1 m^2-1 ha", "1-10000 ha")
inds <- list(4:6, 1:6, 7:10, 11:l)
sapply(1:length(inds), function(x) {
  v <- round(sum(hdatst[inds[[x]]]), 1)
  names(v) <- hds[x]
  v
})

# extent
# hdatst <- hdatstat[[2]]$density
hdatst <- hdatstat_bt$ext$mu
l <- length(hdatst)
hds <- c("<1 ha", "<10 ha", "<100 ha", "<1000ha", "<10Kha", "10-1k ha", "1k-10k",  
         "10k-100k", "100k-1Mk", ">1Kha", ">100Kha", ">1Mha")
inds <- list(1:6, 1:7, 1:8, 1:9, 1:10, 8:9, 10, 11, 12, 10:l, 12:l, 13:l)
sapply(1:length(inds), function(x) {
  v <- round(sum(hdatst[inds[[x]]]), 1)
  names(v) <- hds[x]
  v
})

# interval
# hdatst <- hdatstat[[3]]$density
hdatst <- hdatstat_bt$int$mu
l <- length(hdatst)
hds <- c("norep", "<daily", "<weekly", "<minute", "min-hour", 
         "hr-day", "day-wk", "wk-mo", "mo-yr", "yr-dec", "dec-cen", "cen-mil")
inds <- list(l, 1:3, 1:4, 1, 2, 3, 4, 5, 6, 7, 8, 9)
sapply(1:length(inds), function(x) {
  v <- round(sum(hdatst[inds[[x]]]), 1)
  names(v) <- hds[x]
  v
})

# duration
# hdatst <- hdatstat[[4]]$density
hdatst <- hdatstat_bt$dur$mu
l <- length(hdatst)
hds <- c("<=daily", "day-mo", "mo-yr", "yr-dec", "yr-cent")
inds <- list(1:3, 4:5, 6, 7, 8:10)
sapply(1:length(inds), function(x) {
  v <- round(sum(hdatst[inds[[x]]]), 1)
  names(v) <- hds[x]
  v
})

# types of observations
# unique(datf$study_type)
datf[, round(.N / nrow(datf) * 100, 2), by = study_type]
round(datf[study_type == "automated" & log10(t_btwn_samp) < -1.380211, .N] / 
        hdat[int < -1.380211, .N] * 100, 2)
```

<a href="#top">Back to top</a>

### Figure 2 
```{r, eval = FALSE}
bootp_dt <- rbindlist(bootperturb)

# choose which dataset to use
# kdat <- datf  # ordinary
kdat <- bootp_dt  # bootstrap
resdat3 <- data.table(x = log10(to_infin_byond(kdat$t_btwn_samp)), 
                      y = log10(kdat$plot_res))
resdat3 <- resdat3[round(x, 8) != 6.56229286]  # remove one-time observations
resdat3[, n := 1]  # set up count column
# resdat3$n <- rep(1, nrow(resdat3))
ilim <- range(resdat3$x)
ilim <- round(ilim / 0.5) * 0.5  # round to nearest 0.5
ilim[1] <- -5  # set min interval to less than second
#rx[2] <- 6.56
ilim[2] <- 5.57  # set max interval to millenium
resdat3[x < ilim[1], x := ilim[1]]
resdat3[x > ilim[2], x := ilim[2]]
rlim <- range(aaxis1$logres)
rlim[1] <- -5  # set limits to 0.01 cm^2
resdat3[y < rlim[1], y := rlim[1]]
resdat3[y > rlim[2], y := rlim[2]]
coordinates(resdat3) <- ~x + y

# extent
extdat3 <- data.table(x = log10(kdat$eff_dur),#log10(kdat$act_dur), 
                      y = log10(kdat$eff_ext))#log10(kdat$act_ext))
extdat3 <- extdat3[!is.na(x) & !is.na(y)] # cut out null values
# extdat3$n <- rep(1, nrow(extdat3))
extdat3[, n := 1]  # set up count column
dlim <- range(taxis2$logdays)
dlim <- c(floor(dlim[1]), 5.56) ### should this be 5.57?  #ceiling(ex[2]))
extdat3[x < dlim[1], x := dlim[1]]
extdat3[x > dlim[2], x := dlim[2]]
elim <- range(aaxis2$logarea)
extdat3[y < elim[1], y := elim[1]]
extdat3[y > elim[2], y := elim[2]]
coordinates(extdat3) <- ~x + y

# temporal framing (interval-duration)
tempdat <- data.table(x = log10(to_infin_byond(kdat$t_btwn_samp)), 
                      y = log10(kdat$eff_dur))#kdat$act_dur))
tempdat <- tempdat[!is.na(x) & !is.na(y)] # cut out null values
tempdat <- tempdat[round(x, 8) != 6.56229286, ]  # remove one-time observations

# tempdat$n <- rep(1, nrow(tempdat))
tempdat[, n := 1]
# tx <- range(tempdat$x)
# tx <- round(rx / 0.5) * 0.5  # round to nearest 0.5
tempdat[x < ilim[1], x := ilim[1]]
tempdat[x > ilim[2], x := ilim[2]]
dlimty <- range(taxis2$logdays)
# ty <- c(floor(ty[1]), 5.56)#ceiling(ex[2]))
# ty <- c(floor(ty[1]), 6.6)  # just above 10 KA
tempdat[y < elim[1], y := elim[1]]
tempdat[y > elim[2], y := elim[2]]
coordinates(tempdat) <- ~x + y

# spatial framing (res-extent)
spatdat <- data.table(x = log10(kdat$plot_res), y = log10(kdat$eff_ext))#act_ext))
spatdat <- spatdat[!is.na(x) & !is.na(y)] # cut out null values
# spatdat$n <- rep(1, nrow(spatdat))
spatdat[, n := 1]
# sx <- range(aaxis1$logres)
sx[1] <- -5  # set limits to 0.01 cm^2
spatdat[x < rlim[1], y := rlim[1]]
spatdat[x > rlim[2], y := rlim[2]]
# sy <- range(aaxis2$logarea)
spatdat[y < elim[1], y := elim[1]]
spatdat[y > elim[2], y := elim[2]]
coordinates(spatdat) <- ~x + y

# # interval-extent
# iextdat <- data.table(x = log10(kdat$t_btwn_samp), y = log10(kdat$eff_ext))
# iextdat <- iextdat[!is.na(x) & !is.na(y)] # cut out null values
# iextdat <- iextdat[round(x, 8) != 6.56229286, ]  # remove one-time observations
# iextdat[, n := 1]
# iextdat[x < ilim[1], x := ilim[1]]
# iextdat[x > ilim[2], x := ilim[2]]
# iextdat[y < elim[1], y := elim[1]]
# iextdat[y > elim[2], y := elim[2]]
# coordinates(iextdat) <- ~x + y
# 
# # resolution-duration
# rdurdat <- data.table(x = log10(kdat$eff_dur), y = log10(kdat$plot_res))
# rdurdat <- rdurdat[!is.na(x) & !is.na(y)] # cut out null values
# rdurdat[, n := 1]
# rdurdat[x < dlim[1], x := dlim[1]]
# rdurdat[x > dlim[2], x := dlim[2]]
# rdurdat[y < rlim[1], y := rlim[1]]
# rdurdat[y > rlim[2], y := rlim[2]]
# coordinates(rdurdat) <- ~x + y

# densities
# interval-resolution
rres3 <- kdensity(ilim[1], ilim[2], rlim[1], rlim[2], 0.1, resdat3, 1)
rres3 <- (rres3 / cellStats(rres3, sum)) * 100

# duration-extent
rext3 <- kdensity(dlim[1], dlim[2], elim[1], elim[2], 0.1, extdat3, 1)
rext3 <- (rext3 / cellStats(rext3, sum)) * 100

# interval-duration
tres <- kdensity(ilim[1], ilim[2], dlim[1], dlim[2], 0.1, tempdat, 1)
tres <- (tres / cellStats(tres, sum)) * 100

# resolution-extent
sres <- kdensity(rlim[1], rlim[2], elim[1], elim[2], 0.1, spatdat, 1)
sres <- (sres / cellStats(sres, sum)) * 100

# # interval-extent
# iext <- kdensity(ilim[1], ilim[2], elim[1], elim[2], 0.1, iextdat, 1)
# iext <- (iext / cellStats(iext, sum)) * 100
# 
# # resolution-duration
# rdur <- kdensity(dlim[1], dlim[2], rlim[1], rlim[2], 0.1, rdurdat, 1)
# rdur <- (rdur / cellStats(rdur, sum)) * 100

brkfun <- function(ext, ival, n) {
  rng <- range(ext[is.finite(ext)])
  bwidth <- (rng[2] - rng[1]) / n
  brks <- seq(rng[1], rng[2], bwidth)
  brklabs <- seq(0, round(rng[2], 2), ival)
  list("brks" = brks, "labs" = brklabs)
}

# cxa = 0.7
ll <- 3.5
cxa = 0.8
stps <- sapply(c(3, 4), function(x) which(kdat$st == x))  # IDs rs/auto/paleo
pchs <- list(1, "+")
cexs <- c(0.4, 0.6)
cuts <- 40
bump <- 3
cols <- inferno(cuts + bump)[-c(2:(bump + 1))]
# plot(1:cuts, col = cols, pch = 20)

# pdf("paper/figures/res_v_extent.pdf", width = 7, height = 2.5)
pdf("paper/figures/kde45.pdf", width = 7, height = 6)
par(mfrow = c(2, 2), mar = c(6, 6, 0.3, 3), oma = c(0, 0, 0.5, 1), 
    mgp = c(2, 0.25, 0))
# resolutions
# image(rres3, col = rev(terrain.colors(30)), breaks = brks,
image(rres3, axes = FALSE, xlab = "", ylab = "", #rev(terrain.colors(cuts))) 
      col = cols)
axis(1, at = taxis1$logdays[-c(5)], labels = tlab1[-c(5)], 
     las = 2, tcl = -0.2, cex.axis = cxa)
axis(2, at = aaxis1$logres[-1], labels = alab1[-1], las = 2, tcl = -0.2, 
     cex.axis = cxa)
mtext("Interval", 1, cex = cxa, line = ll)
mtext("Resolution", 2, cex = cxa, line = ll)
brks <- brkfun(rres3, 0.01, cuts)  # breaks
aargs <- list(mgp = c(3, 0.25, 0), at = brks$labs, labels = brks$labs, 
              cex.axis = cxa, tcl = -0.1)
plot(rres3, legend.only = TRUE, axis.args = aargs, legend.width = 1.25,
     #legend.shrink = 0.9, breaks = brks, col = rev(terrain.colors(30)))
     legend.shrink = 0.9, col = cols)#inferno(cuts))
     #col = rev(terrain.colors(cuts)))
mtext("A", side = 3, line = -1, cex = 0.8, adj = 0.05, col = "white")

# extents
image(rext3, axes = FALSE, xlab = "", ylab = "", col = cols)
axis(1, at = taxis2$logdays[-5], labels = tlab2[-5], las = 2,
     cex.axis = cxa, tcl = -0.2)
axis(2, at = aaxis2$logarea, labels = alab2, las = 2, cex.axis = cxa,
     tcl = -0.2)
brks <- brkfun(rext3, 0.01, cuts)  # breaks
aargs <- list(mgp = c(3, 0.25, 0), at = brks$labs, labels = brks$labs, 
              cex.axis = cxa, tcl = -0.1)
mtext("Duration", 1, cex = cxa, line = ll)
mtext("Extent", 2, cex = cxa, line = ll)
plot(rext3, legend.only = TRUE, axis.args = aargs, legend.width = 1.25,
     legend.shrink = 0.9, col = cols)
mtext("B", side = 3, line = -1, cex = 0.8, adj = 0.05, col = "white")

# spatial
image(sres, col = cols, axes = FALSE, xlab = "", ylab = "") 
axis(1, at = aaxis1$logres, labels = alab1, las = 2, tcl = -0.2, cex.axis = cxa)
axis(2, at = aaxis2$logarea, labels = alab2, las = 2, tcl = -0.2, 
     cex.axis = cxa)
mtext("Resolution", 1, cex = cxa, line = ll)
mtext("Extent", 2, cex = cxa, line = ll)
brks <- brkfun(sres, 0.01, cuts)  # breaks
aargs <- list(mgp = c(3, 0.25, 0), at = brks$labs, labels = brks$labs, 
              cex.axis = cxa, tcl = -0.1)
plot(sres, legend.only = TRUE, axis.args = aargs, legend.width = 1.25,
     legend.shrink = 0.9, col = cols)
mtext("C", side = 3, line = -1, cex = 0.8, adj = 0.05, col = "white")

# temporal
image(tres, axes = FALSE, xlab = "", ylab = "", col = cols)
for(i in 1:2) {  # using taxis2 values for both axes because same
  axis(i, at = taxis2$logdays[-5], labels = tlab2[-5], las = 2,
       cex.axis = cxa, tcl = -0.2)
}
mtext("Interval", 1, cex = cxa, line = ll)
mtext("Duration", 2, cex = cxa, line = ll)
# brks <- brkfun(tres, 0.02, 100)  # breaks
brks <- brkfun(tres, 0.01, cuts)  # breaks
aargs <- list(mgp = c(3, 0.25, 0), at = brks$labs, labels = brks$labs, 
              cex.axis = cxa, tcl = -0.1)
plot(tres, legend.only = TRUE, axis.args = aargs, legend.width = 1.25,
     legend.shrink = 0.9, col = cols)
mtext("D", side = 3, line = -1, cex = 0.8, adj = 0.05, col = "white")
dev.off()

# image(rdur, axes = FALSE, xlab = "", ylab = "", col = cols)
# plot(rdur, legend.only = TRUE, axis.args = aargs, legend.width = 1.25,
#      legend.shrink = 0.9, col = cols)

save(bootp_dt, file = fp(p_dat, "derived/bootresults.rda"))
```

<a href="#top">Back to top</a>

### Figure 3

Actual versus effective duration/extent
```{r, eval=FALSE}

# plot params
ex <- range(aaxis2$logarea)
ey <- range(aaxis2$logarea)
ll <- 3.5
cxa = 0.7
stps <- sapply(c(3, 4), function(x) which(kdat$st == x))  # IDs rs/auto/paleo
pchs <- list(1, "+")
cexs <- c(0.4, 0.6)
cuts <- 40
bump <- 3
cols <- inferno(cuts + bump)[-c(2:(bump + 1))]

# Difference in effective versus actual extent/duration against actual ext/dur
hdat_bs2 <- lapply(1:length(bootperturb), function(x) {  # x <- 1
  print(x)
  DT <- bootperturb[[x]]
  
  # extent measures log10 transformed
  edb <- DT[, lapply(.SD, log10), .SDcol = c("eff_ext", "act_ext")]
  setnames(edb, c("eff_ext", "act_ext"), c("eext", "aext"))
  edb[, dif := eext - aext]  # difference between extent and actual extent

  # duration measures log10 transformed, unreplicated dropped
  tdb <- DT[t_btwn_samp != 365 * 10000, lapply(.SD, log10), 
            .SDcol = c("eff_dur", "act_dur")]
  setnames(tdb, c("eff_dur", "act_dur"), c("edur", "adur"))
  # print(paste(edb[, .N], tdb[, .N]))
  tdb[, dif := edur - adur]  # difference between duration and actual duration
  return(list("edb" = edb, "tdb" = tdb, #"elm" = elm, "tlm" = tlm, 
              "Ns" = c(edb[, .N], tdb[, .N])))
})

# calculate box statistics
# Extent differences
eint <- aaxis2$logarea
edif_int <- rbindlist(lapply(hdat_bs2, function(y) {  # y <- hdat_bs2[[2]]
  dd <- copy(y$edb[!is.na(aext)])
  dd[aext < ex[1], aext := ex[1]]  # set to min
  # dd[eext < ex[1], eext := ex[1]]  # set to min
  dd[, int := findInterval(aext, eint, rightmost.closed = TRUE)]
  # dd[, int := findInterval(eext, eint, rightmost.closed = TRUE)]
  # dd[, list("dif" = mean(dif, na.rm = TRUE), "N" = .N), by = int][order(int)]
}))
ints <- sort(edif_int[, unique(int)])
ebox <- lapply(ints, function(x) edif_int[int == x, lmisc::box_stats(dif)])
pctobs <- sapply(ints, function(x) edif_int[int == x, .N])
ewgt <- pctobs / sum(pctobs)
# round(cumsum(ewgt), 2)
ewgtint <- c(0, 1, 5, 10, 15, 20, 25) / 100  # weight intervals
ewgttxt <- c("  0", "  1", "  5", seq(10, 25, 5))
ecols <- findInterval(ewgt, ewgtint)  # corresponding color index

# temporal differences
dint <- taxis2$logdays[-5]
tx <- range(dint)
ddif_int <- rbindlist(lapply(hdat_bs2, function(y) {  # y <- hdat_bs2[[2]]
  dd <- copy(y$tdb[!is.na(adur)])
  dd[adur < tx[1], adur := tx[1]]  # set to min
  dd[adur > tx[2], adur := tx[2]]  # set to max
  dd[, int := findInterval(adur, dint, rightmost.closed = TRUE)]  # include high
}))
dints <- sort(ddif_int[, unique(int)])
tbox <- lapply(dints, function(x) ddif_int[int == x, lmisc::box_stats(dif)])
pctobs <- sapply(dints, function(x) ddif_int[int == x, .N])
dwgt <- pctobs / sum(pctobs)
# round(cumsum(dwgt), 2)

dwgtint <- c(0, 1, 5, 10, 15, 20, 25, 30) / 100  # weight intervals
dwgttxt <- c("  0", "  1", "  5", seq(10, 30, 5))
dcols <- findInterval(dwgt, dwgtint)  

# y grid line function
ylines <- function(yr, by, lty = 1, col = "grey") {
  abline(h = seq.int(from = yr[1], to = yr[2], by = 2), lty = lty, col = col)
}

# pdf("paper/figures/act_v_eff_diff.pdf", width = 7, height = 4)
png("paper/figures/act_v_eff_diff.png", width = 7, height = 4, res = 600, 
    units = "in")
par(mfrow = c(1, 2), mar = c(4, 0, 1, 0.75), oma = c(0, 3, 0, 0))
bx <- rowMeans(cbind(eint[-length(eint)], eint[-1]))
# cols <- sapply(ealphs, function(x) rgb(0, 0.1, 1, alpha = x))
cols <- brewer.pal(length(ewgtint) - 1, "Blues")
plot(ex, c(0, 14), pch = "", xlab = "", ylab = "", xaxt = "n", yaxt = "n",
     mgp = c(3, 0.5, 0), tcl = -0.2, xaxs = "i")
polygon(c(ex, rev(ex), ex[1]), c(-1, -1, 15, 15, -1), col = "grey90")
ylines(yr = c(0, 14), by = 2, col = "grey95")
for(i in 1:length(ebox)) {
  boxplot_v(bx[i], y = ebox[[i]], n = 1.75, whiskhgt = 1.5, 
            bfill = cols[ecols][i])
  text(bx[i], ebox[[i]][5] + 0.15, round(ewgt[i] * 100, 1), cex = 0.3)
} 
axis(1, at = eint, labels = alab2, las = 2, tcl = -0.2, cex.axis = cxa, 
     mgp = c(3, 0.5, 0))
axis(2, at = seq(0, 14, 2), las = 2, tcl = -0.2, cex.axis = cxa, 
     mgp = c(3, 0.5, 0))
mtext("Actual Extent", side = 1, line = 3, cex = 0.9)
mtext("Difference between Extent/Duration", side = 2, line = 2.2, cex = 0.9)
mtext("and Actual Extent/Duration (Decades)", side = 2, line = 1.4, cex = 0.9)
lrng <- c(mean(ex), max(ex) - 0.05 * max(ex))
xs <- seq(lrng[1], lrng[2], by = (diff(lrng)) / (length(cols)))
rect_shade(xs, y = c(11.5, 12.5), fillcol = cols, linecol = "grey30")
text(xs, rep(10.5, length(xs)), ewgttxt, srt = 90, col = "grey30")
text(mean(xs), 13, "% of observations", col = "grey30")
mtext("A", side = 3, line = -1, cex = 1, adj = 0.02)

# duration
bx <- rowMeans(cbind(dint[-length(dint)], dint[-1]))
bd <- abs(dint[-length(dint)] - dint[-1])
cols <- brewer.pal(length(dwgtint) - 1, "Reds")
plot(tx, c(0, 14), pch = "", xlab = "", ylab = "", xaxt = "n", xaxs = "i",
     mgp = c(3, 0.5, 0), tcl = -0.2, yaxt = "n")
polygon(c(tx, rev(tx), tx[1]), c(-1, -1, 15, 15, -1), col = "grey90")
ylines(yr = c(0, 14), by = 2, col = "grey95")
for(i in 1:length(tbox)) {
  boxplot_v(bx[i], y = tbox[[i]], n = 0.9, inhgt = bd[i] * 10, whiskhgt = 0.75, 
            bfill = cols[dcols][i])
  text(bx[i], tbox[[i]][5] + 0.15, round(dwgt[i] * 100, 1), cex = 0.3)

} 
axis(1, at = dint, labels = tlab2[-c(5)], las = 2, tcl = -0.2, cex.axis = cxa, 
     mgp = c(3, 0.5, 0))
axis(2, at = seq(0, 14, 2), labels = rep("", 8), las = 2, tcl = -0.2, 
     cex.axis = cxa, mgp = c(3, 0.5, 0))
mtext("Actual Duration", side = 1, line = 3, cex = 0.9)
lrng <- c(mean(tx), max(tx) - 0.05 * max(tx))
xs <- seq(lrng[1], lrng[2], by = (diff(lrng)) / (length(cols)))
rect_shade(xs, y = c(11.5, 12.5), fillcol = cols, linecol = "grey30")
text(xs, rep(10.5, length(xs)), dwgttxt, srt = 90, col = "grey30")
text(mean(xs), 13, "% of observations", col = "grey30")
mtext("B", side = 3, line = -1, cex = 1, adj = 0.02)
dev.off()

```

## Additional Analyses

### Sensitivity
Just for the four main variables. Rules: 

+ If `plot_res` and/or `n_sites` uncertain, then `act_ext` must be. Recalculate using random draw within the uncertain variable 
+ If `t_btwn_samp` and/or `samp_dur` uncertain, do the same as above, but we have no variable for n repeats for calculating `act_dur`.

```{r}
svars <- c("plot_res", "n_sites", "act_ext", "eff_ext", "samp_dur",
           "t_btwn_samp", "act_dur", "eff_dur")
sens <- gsub("\\\\", ";", dat$sensitivity)
sens <- gsub(", ", ";", gsub("; ", ";", sens))
sens <- tolower(sens)
sensl <- strsplit(sens, ";")
# unique(unlist(sensl))

sens2 <- gsub("plo_|plot-", "plot_", sens)  # fix this one
sens2 <- gsub("[[:blank:]]", "_", sens2)  # replace spaces with _
sens2 <- gsub("-", "_", sens2)  # replace - with _

corrtab <- cbind(c("^res", "plo_res", "plot_resolution", "plot_size", 
                   "sample_area", "samples_area", "sampled area", "study_area",
                   "sampled_area",
                   "t_btwn_samples", "t_btwn_sample", "t_btwn_samp_samp", 
                   "t_twn_samp", "t_btw_samp", "time_between_sampling", 
                   "time_between_samples", 
                   "samp_duratiion", "sampling_duration", "sample_duration", 
                   "number_of_sites", 
                   "study_span", "sutdy_span", "study_dur", "study_soan",
                   "samp_study", ":", " ", ";_", 
                   "duration", "dure", 
                   "extent"), 
                 c(rep("plot_res", 4), rep("act_ext", 5), rep("t_btwn_samp", 7),                    rep("samp_dur", 3), "n_sites", rep("eff_dur", 4), 
                   "samp;study", ";", ";", ";", 
                   "dur", "dur", "ext"))

# replace
for(i in 1:nrow(corrtab)) {
  sens2 <- gsub(corrtab[i, 1], corrtab[i, 2], sens2)
}
# gsub("[a-z]( )[a-z]", "_", sens[[228]])
# sens[[228]]
# sens2[is.na(sens2)]
sens2[is.na(sens2)] <- "none"
sensl2 <- lapply(1:length(sens2), function(x) strsplit(sens2[x], ";")[[1]])
# unique(unlist(sensl))
# unique(unlist(sensl2))

# create a sensitivity table
senst <- do.call(cbind.data.frame, lapply(svars, function(x) {
  v <- sapply(sensl2, function(y) ifelse(any(y == x), 1, 0))
  # ifelse(is.na(v), 0, v)
}))
colnames(senst) <- c("res", "n", "aext", "eext", "sampd", "tbtwn", "actdur", 
                     "effdur")

# apply fixes (mostly to account for observer omissions)
# 1. if samp_dur OR t_btwn_samp is uncertain, act_dur must also be. 
id <- which((senst$sampd == 1 | senst$tbtwn == 1) & senst$actdur == 0)
senst[id, "actdur"] <- 1

# 2. if act_dur is uncertain but t_btwn and samp_dur are not, treat
# both as uncertain
# although it is possible that just n_repeats is uncertain, but more likely 
# that observers didn't note this is sensitive. 
id <- which(senst$tbtwn == 0 & senst$sampd == 0 & senst$actdur == 1)
senst[id, c("sampd", "tbtwn")] <- 1

# 3. if plot_res or n_sites is uncertain, then act_ext must be
id <- which((senst$res == 1 | senst$n == 1) & senst$aext == 0)
senst[id, "aext"] <- 1

# percentage of records that are uncertain
round(colSums(senst) / nrow(senst) * 100, 1)

```

<a href="#top">Back to top</a>

### Study type by year

Calculate types of study by year, as total and percent
```{r, warning = FALSE, message = FALSE, fig.height=3}
# One Oecologia study listed as 2004 (by Endnote) was actually 2003 
# Set it to 2004
stypes <- copy(dat)  # new dataset out of dat (which retain year info)
stypes[study_year < 2004, study_year := 2004] 

# bring in study_type from calr set, grabbing year from original cal dt
calyr <- cal[DOI %in% calr[, unique(DOI)], unique(study_year), by = DOI]
calyr[, V1 := as.numeric(V1)]
calyr <- calyr[, list("study_year" = mean(V1, na.rm = TRUE)), by = DOI]
calyr <- merge(calyr, calr[, list(DOI, study_type)], by = "DOI")

# bind
stypes <- rbind(stypes[, list(DOI, study_year, study_type)], calyr)
na_effext <- datf[is.na(eff_ext), unique(DOI)]  # ID NA eff_ext values

# Estimate and plot study type by year
stypeyr <- stypes[, list(.N, study_type), by = study_year][, {
  list("tot" = N, "ct" = .N, "prop" = .N / N * 100)
}, by = .(study_type, study_year)][order(study_year)]
stypeyr <- stypeyr[, lapply(.SD, mean), .SDcols = c("tot", "ct", "prop"), 
                   by = .(study_type, study_year)]
```
```{r, eval = FALSE}
obs_types <- c("remote", "field", "auto")
obs_types2 <- c("Remote sensing", "Field", "Automatic")
cols <- c("orange", "green4", "blue4")
png(fp(p_fig, "stype_by_yr.png"), width = 7, height = 2.75, res = 300, 
    units = "in")
par(mfrow = c(1, 3), oma = c(0, 3, 0, 0), mar = c(4, 1, 2, 1))
for(i in 1:3) {  # i <- 1
  plot(c(2004, 2014), c(0, 100), xlab = "", ylab = "", pch = "")
  stypeyr[like(study_type, obs_types[i]), {
    points(study_year, prop, col = cols[i], pch = 20)
    yr = as.numeric(study_year)
    slm = lm(prop ~ yr, weights = tot)  # weighted regression
    slms = summary(slm)
    abline(slm, col = cols[i])
    cf = round(coef(slm)[1:2], 4)
    pf = round(coef(slms)[8], 2)
    rf = round(slms$adj.r.squared, 2)
    cfa = paste0(cf[2], "% year")
    text(x = 2006, y = 100, labels = substitute(x^-1, list(x = cfa)), 
         cex = 0.9)
    text(x = 2011.5, y = 100, cex = 0.9,
         labels = substitute(R^2~"="~x~"; p <"~y, list(x = rf, y = pf)))
    if(i == 1) mtext("percent", side = 2, line = 2.5, cex = 0.8)
    if(i == 2) mtext("year", side = 1, line = 2.5, cex = 0.8)
    mtext(obs_types2[i], side = 3, cex = 0.8)
  }]
}
dev.off()
```

Extrapolating percentage of studies using remote sensing to 2017
```{r}
preds <- stypeyr[like(study_type, "remote"), {
  slm = lm(prop ~ study_year, weights = tot)
  predict(slm, newdata = data.frame("study_year" = 2004:2017))
}]
names(preds) <- 2004:2017
```

Percent of remote sensing studies by year
```{r}
round(preds, 1)
```

Average annual percent of studies using remote sensing 2004-2014, and projected increases in this percent through 2017 (2004-2017) 
```{r}
# murs2014 <- stypeyr[study_type == "remote", round(mean(prop), 2)]
murs2014 <- stypeyr[, sum(ct[study_type == "remote"]) / sum(ct) * 100]
pctinc <- mean(preds) / mean(preds[1:(length(preds) - 3)])

pcttab <- cbind("2014" = murs2014, "pct increase" = (pctinc - 1) * 100,
                "2017" = murs2014 * pctinc)
round(pcttab, 1)
```

```{r}
types <- datf[, .N, by = study_type]
rs2017 <- mean(preds)  # avg predicted % using RS through 2004-2017
rs2014 <- mean(preds[1:(length(preds) - 3)])  # avg predicted % RS 2004-2014
# rs2017 / rs2014

# translate to N more RS studies would have been selected given those %ages
rsinc <- rs2017 / rs2014 * types[study_type == "remote", N] - 
                 types[study_type == "remote", N]  

remote <- datf[study_type == "remote", eff_ext]
notremote <- datf[study_type != "remote", eff_ext]

set.seed(1)
ext2017 <- unlist(lapply(1:1000, function(x) {  # x = 1
  ind <- sample(1:length(remote), size = round(rsinc))
  mean(c(remote, remote[ind], notremote), na.rm = TRUE)
}))

dec1417 <- log10(mean(ext2017, na.rm = TRUE) / 
                   datf[, mean(eff_ext, na.rm = TRUE)])
qtiles1417 <- log10(quantile(ext2017, c(0.025, 0.975), na.rm = TRUE) / 
                      datf[, mean(eff_ext, na.rm = TRUE)])

pctdiff1417 <- mean(ext2017, na.rm = TRUE) / datf[, mean(eff_ext, na.rm = TRUE)]
pctqtiles1417 <- quantile(ext2017, c(0.025, 0.975), na.rm = TRUE) / 
                      datf[, mean(eff_ext, na.rm = TRUE)]

cbind("decade" = round(dec1417, 4), "dec2.5" = round(qtiles1417[1], 4),
      "dec97.5" = round(qtiles1417[2], 4),
      "pctdiff" = round((pctdiff1417 - 1) * 100, 1),
      "pctdiff2.5" = round((pctqtiles1417[1] - 1) * 100, 1), 
      "pctdiff97.5" = round((pctqtiles1417[2] - 1) * 100, 1))
```

### Change in dimensions by year
```{r}
# Examines and plot trends in dimensions
doiyr <- rbind(unique(dat[, .(DOI, study_year)]), 
               unique(cal[, .(DOI, study_year)]))
doiyr <- doiyr[study_year %in% 2004:2014]
datf2 <- copy(datf)
datf2 <- merge(doiyr, datf2, by = "DOI")  # collect year of publication
# datf2[, N := .N, by = study_year]
```
```{r, eval = FALSE}
vars <- c("plot_res", "eff_ext", "t_btwn_samp", "eff_dur")
varlabs <- c("Resolution", "Extent", "Interval", "Duration")
labcols <- c(brewer.pal(8, "Reds")[c(7, 7)], brewer.pal(8, "Blues")[c(7, 7)])
png(fp(p_fig, "dim_by_yr.png"), width = 7, height = 6, res = 300, 
    units = "in")
par(mfrow = c(2, 2), mar = c(1, 3, 1, 1), oma = c(2, 3, 0, 0))
for(i in 1:length(varlabs)) {  # i <- 1
  if(i == 3) {
    DT <- copy(datf2[t_btwn_samp != 0, ])
  } else {
    DT <- copy(datf2)
  }
  DT[, list(mean(get(vars[i]), na.rm = TRUE), .N), by = study_year][, {
    dim = log10(V1)
    yr = as.numeric(study_year)
    if(i %in% 1:2) rng = c(0, 10)
    if(i %in% 3:4) rng = c(0, 5)
    if(i %in% 3:4) {
      xlabs = seq(2004, 2014, 2)
    } else {
      xlabs = rep("", 6)
    }
    plot(yr, dim, ylim = rng, xaxt = "n", pch = 20, col = labcols[i], 
         main = varlabs[i], mgp = c(2, 0.5, 0), tcl = -0.2, ylab = "")
    axis(1, at = seq(2004, 2014, 2), labels = xlabs, tcl = -0.2, 
         mgp = c(2, 0.5, 0))
    slm = lm(dim ~ yr, weights = N)
    slms = summary(slm)
    abline(slm, col = labcols[i])
    cf = round(coef(slm)[1:2], 3)
    pf = round(coef(slms)[8], 2)
    rf = round(slms$adj.r.squared, 2)
    text(x = 2006.25, y = rng[2] * 0.97, cex = 0.9,
         labels = substitute(x~year^-1, list(x = cf[2])))
    text(x = 2011.5, y = rng[2] * 0.97, cex = 0.9,
         labels = substitute(R^2~"="~x~"; p <"~y, list(x = rf, y = pf)))
    if(i == 1) mtext(expression("log10 m"^2), side = 2, line = 2)
    if(i == 2) mtext("log10 ha", side = 2, line = 2)
    if(i %in% 3:4) mtext("log10 days", side = 2, line = 2)
  }]
}
dev.off()
```

Percent increase in average extent by year
```{r}
preds <- datf2[, list(mean(eff_ext, na.rm = TRUE), .N), by = study_year][, {
  dim = log10(V1)
  yr = as.numeric(study_year)
  slm = lm(dim ~ yr, weights = N)
  predict(slm, newdata = data.frame("yr" = 2004:2017))
}]
# preds[14] - preds[13]

# projected percent increase in extent 2014-2017
pctinc <- mean(preds) / mean(preds[1:(length(preds) - 3)])
muext2014 <- datf2[, mean(eff_ext, na.rm = TRUE)]
pcttab <- cbind("2014" = log10(muext2014), "pct increase" = (pctinc - 1) * 100,
                "2017" = log10(muext2014 * pctinc), 
                "decade" = log10((pctinc * muext2014) / muext2014))
round(pcttab, 3)
```

### Scales in earlier studies
#### Duration from Tilman, 1989
```{r, echo=FALSE}
load(fp(p_dat, "derived/bootresults.rda"))
```
```{r}
std <- function(x) (x - min(x)) / diff(range(x))

# Tilman (1989), Figure 6.1 (visually interpreted)
# Data extracted from figure using http://arohatgi.info/WebPlotDigitizer/app/
tilman <- fread(fp(p_til, "tilman1989.csv"))

# Find n and proportion of non-experimental observations per year
# find mean/median value of data > 100 years in our observed data
gt100 <- bootp_dt[eff_dur > (365 * 100), {
  list("med" = median(eff_dur) / 365, "mu" = mean(eff_dur) / 365) 
}]

# find weights and proportion/year
tilman_noexp <- tilman[, {
  a = round(623 * prop[fig == 1] * (1 / sum(prop[fig == 1])))
  b = round(180 * prop[fig == 2] * (1 / sum(prop[fig == 2])))
  padj = a - b
  w = padj / sum(padj)
  list("yrmu" = yrmu[fig == 1], "n" = padj, "prop" = w)
}]

# median and mean per year, assuming 150 years for studies > 100
tilmedmu <- tilman_noexp[, rep(yrmu, n), by = n][, {
  list("med" = median(V1), "mean" = mean(V1))
}]

# test additional upper bin years
gt1002 <- unname(c(200, 500, 1000, unlist(gt100)))
tilmedmu2 <- rbindlist(lapply(gt1002, function(x) {
  DT2 = copy(tilman_noexp)[yrmu == 150, yrmu := x]
  DT2[, rep(yrmu, n), by = n][, list("med" = median(V1), "mean" = mean(V1))]
}))

durmedmu <- bootp_dt[, {
  list("med" = median(eff_dur[t_btwn_samp != (365 * 10000)], na.rm = TRUE) / 365, 
       "mean" = mean(eff_dur[t_btwn_samp != (365 * 10000)], na.rm = TRUE) / 365, 
       "medall" = median(eff_dur, na.rm = TRUE) / 365, 
       "meanall" = mean(eff_dur, na.rm = TRUE) / 365)
}]

# And calculate only for years less than 100
tilmedmu_lt100 <- tilman_noexp[yrmu < 150, rep(yrmu, n), by = n][, {
  list("med" = median(V1), "mean" = mean(V1))
}]

durmedmu_lt100 <- bootp_dt[eff_dur < (365 * 100), {
  list("med" = median(eff_dur[t_btwn_samp != (365 * 10000)], na.rm = TRUE) / 365, 
       "mean" = mean(eff_dur[t_btwn_samp != (365 * 10000)], na.rm = TRUE) / 365, 
       "medall" = median(eff_dur, na.rm = TRUE) / 365, 
       "meanall" = mean(eff_dur, na.rm = TRUE) / 365)
}]

```

Median, mean duration from Tilman (1989)
```{r}
round(rbind(tilmedmu, tilmedmu2), 2)
```

Tilman with studies > 100 years dropped
```{r}
round(tilmedmu_lt100, 2)
```

Tilman (1989) duration of observational studies by bin
```{r}
tilman_noexp[, {
  a = round(prop * 100, 1)
  names(a) = tilman[fig == 1, yr]
  rbind(a, n)
}]
```

Median, mean duration from this study
```{r}
round(durmedmu, 2)
```

This study with years > 100 dropped
```{r}
round(durmedmu_lt100, 2)
```

#### Kareiva and Anderson (1989)

Resolution versus this study
```{r}
# helper func
q80 <- function(x) quantile(x, 0.8, na.rm = TRUE)
q90 <- function(x) quantile(x, 0.9, na.rm = TRUE)
q95 <- function(x) quantile(x, 0.95, na.rm = TRUE)

kareiva <- fread(fp(p_til, "kareiva_anderson1989.csv"))
a <- kareiva[, {
  list("med" = median(res^2), "mean" = mean(res^2), "80" = q80(res^2), 
       "90" = q90(res^2), "95" = q95(res^2))
}]
b <- bootp_dt[, {
  list("med" = median(plot_res), "mean" = mean(plot_res), "80" = q80(plot_res), 
       "90" = q90(plot_res), "95" = q95(plot_res))
}]
study <- c("Kareiva & Anderson, 1989", "This study")
cbind(study, round(rbind(a, b), 2))
```

#### Porter et al (2005)
Interval
```{r}
porter <- fread(fp(p_til, "porter.csv"))
porter <- porter[sample == 1]
porter[, ext := ext^2 / 10000]

study <- c("Porter et al, 1989", "This study")
a <- porter[int > 0, {
  list("med" = median(int), "mean" = mean(int), "80" = q80(int), "90" = q90(int),
       "95" = q95(int))
}]

b <- bootp_dt[t_btwn_samp != (365 * 10000), {
  list("med" = median(t_btwn_samp), "mean" = mean(t_btwn_samp), 
       "80" = q80(t_btwn_samp), "90" = q90(t_btwn_samp), "95" = q95(t_btwn_samp))
}]
cbind(study, round(rbind(a, b)))
```

Extent
```{r}
study <- c("Porter et al, 1989", "This study", "Ratio")
a <- porter[, {
  list("med" = median(ext), "mean" = mean(ext), "80" = q80(ext), 
       "90" = q90(ext), "95" = q95(ext))
}] 
b <- bootp_dt[, {
  list("med" = median(eff_ext, na.rm = TRUE), 
       "mean" = mean(eff_ext, na.rm = TRUE), "80" = q80(eff_ext), 
       "90" = q90(eff_ext), "95" = q95(eff_ext))
}]
cbind(study, round(rbind(a, b, b / a)))
```

### Share of remote sensing, automated, and paleo observations
```{r}

aext <- 10^4
datf[, {
  a = length(which(act_ext > aext))
  b1 = length(which((act_ext > aext) & like(study_type, "remote")))
  b2 = length(which((act_ext > aext) & like(study_type, "other")))
  round(c("remote" = b1 / a * 100, "other" = b2 / a * 100), 1)
}]

adur <- 365
datf[, {
  a = length(which(act_dur > adur))
  b1 = length(which((act_dur > adur) & like(study_type, "auto")))
  b2 = length(which((act_dur > adur) & like(study_type, "paleo")))
  round(c("automated" = b1 / a * 100, "paleo" = b2 / a * 100), 1)
}]

datf[, {
  a = length(which(act_dur <= adur))
  b1 = length(which((act_dur <= adur) & like(study_type, "auto")))
  b2 = length(which((act_dur <= adur) & like(study_type, "paleo")))
  round(c("automated" = b1 / a * 100, "paleo" = b2 / a * 100), 1)
}]

```

### Varying kernel sizes
```{r, eval = FALSE}
# redo range variables
rx <- range(resdat3$x)
rx <- round(rx / 0.5) * 0.5  # round to nearest 0.5
rx[1] <- -5  # set min interval to less than second
rx[2] <- 5.57  # set max interval to millenium
ex <- range(taxis2$logdays)
ex <- c(floor(ex[1]), 5.56) ### should this be 5.57?  #ceiling(ex[2]))
ey <- range(aaxis2$logarea)

# density rasters
# vary the kernel size about them
kernsl <- lapply(c(0.4, 0.7, 1), function(x) {
  rr <- kdensity(rx[1], rx[2], ry[1], ry[2], 0.1, resdat3, x)
  rr <- (rr / cellStats(rr, sum)) * 100
  er <- kdensity(ex[1], ex[2], ey[1], ey[2], 0.1, extdat3, x)
  er <- (er / cellStats(er, sum)) * 100
  kerns <- list("res" = rr, "ext" = er)
})

# plot parameters
cxa <- 1.25
cuts <- 40
bump <- 3
cols <- inferno(cuts + bump)[-c(2:(bump + 1))]

# As separate plots
pdf("paper/figures/res_v_extent_ksize2.pdf", width = 8, height = 10)
par(mfrow = c(3, 2), oma = c(6, 3, 0, 0), mar = c(1, 5, 3, 4))
for(i in 1:3) {  # i <- 3
  x <- kernsl[[i]] # x <- kernsl[[1]]
  image(x$res, col = cols, axes = FALSE, xlab = "", ylab = "")
  if(i == 3) {
    axis(1, at = taxis1$logdays[-c(5, 9, 10)], labels = tlab1[-c(5, 9, 10)], 
         las = 2, tcl = -0.2, cex.axis = cxa)
  }
  axis(2, at = aaxis1$logres[-1], labels = alab1[-1], las = 2, tcl = -0.2, 
       cex.axis = cxa)  
  aargs <- list(mgp = c(3, 0.25, 0), #at = brklabs, labels = brklabs, 
                cex.axis = cxa, tcl = -0.1)
  plot(x$res, legend.only = TRUE, axis.args = aargs, legend.width = 1.25,
     legend.shrink = 0.9, col = cols)#breaks = brks, 
  
  image(x$ext, col = cols, axes = FALSE, xlab = "", ylab = "")
  if(i == 3) {
    axis(1, at = taxis2$logdays[-5], labels = tlab2[-5], las = 2,
         cex.axis = cxa, tcl = -0.2)
  }
  axis(2, at = aaxis2$logarea, labels = alab2, las = 2, cex.axis = cxa,
       tcl = -0.2)
  aargs <- list(mgp = c(3, 0.25, 0), #at = brklabs2, labels = brklabs2, 
                cex.axis = cxa, tcl = -0.1)
  plot(x$ext, legend.only = TRUE, axis.args = aargs, legend.width = 1.25,
       legend.shrink = 0.9, col = cols)#breaks = brks2, 
}
dev.off()

```
<a href="#top">Back to top</a>

```{r, eval=FALSE, echo=FALSE}
# check effect of averaging by DOI
datf[, {
 list("a" = log10(mean(plot_res)), "b" = log10(mean(eff_ext, na.rm = TRUE)))
}, by = DOI][, list("a" = mean(a), "b" = mean(b, na.rm = TRUE))]
datf[, list("a" = mean(log10(plot_res)), "b" = mean(log10(eff_ext), na.rm = T))]

```

