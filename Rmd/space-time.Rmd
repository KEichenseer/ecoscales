---
title: "Space-time plots"
author: "Lyndon Estes"
date: "December 3, 2015"
output: 
  html_document:
    toc: yes
    number_sections: true
    toc_depth: 3
---

# Space versus time analysis
```{r, message = FALSE, warning=FALSE, results='hold'}
library(ecoscales)
library(readxl)
# library(stringdist)
library(maptools)
library(RColorBrewer)
library(viridis)
# library(zyp)

p_root <- set_base_path("ecoscales")
p_dat <- fp(p_root, "external/data/result")
p_calib <- fp(p_root, "external/data/calibration")
p_fig <- fp(p_root, "paper/figures")

# dat <- as.data.table(read_excel(full_path(p_dat, "merged.xlsx")))
dat <- as.data.table(read_excel(full_path(p_dat, "mergedffffff.xlsx")))
# dato <- as.data.table(read_excel(full_path(p_dat, "archive/mergedfff.xlsx")))
cal <- data.table(read_excel(fp(p_calib, "merged_calibrationffffff.xlsx")))
full <- data.table(read_excel(fp(p_dat, "full_no_treuer_toss.xlsx")))
# full <- data.table(read_excel(fp(p_dat, "full_less_Treuer.xlsx")))

# setnames(dato, "DOI/title", "DOI")
# dato[, DOI := gsub("DOI:|doi:|DOI: |doi: |DOI ", "", DOI)]
# dato[, DOI := gsub("\\s", "", DOI)]  # remove DOI and whitespace

# clean up DOIs
setnames(dat, "DOI/title", "DOI")
dat[, DOI := gsub("DOI:|doi:|DOI: |doi: |DOI ", "", DOI)]
dat[, DOI := gsub("\\s", "", DOI)]  # remove DOI and whitespace
cal[, DOI := gsub("\\s", "", DOI)]  # remove DOI and whitespace
full <- gsub("DOI:|doi:|DOI: |doi: |DOI ", "", full$DOI)
full <- gsub("\\s", "", full)  # remove DOI and whitespace

# subset main results and define study type
dat <- dat[, names(dat)[1:22], with = FALSE]
dat[, study_type := tolower(study_type)]
# kwords <- c("field", "paleo", "remote", "automated", "other")
# tst <- dat[, study_type]

kwords <- cbind(c("field", "paleo", "remote", "automated", "other"), 
                c("field", "paleo", "remote", "automated", "other"))
# for(i in 1:length(kwords)) dat[like(study_type, kwords[i]), st := i]
for(i in 1:nrow(kwords)) {
  dat[like(study_type, kwords[i, 1]), study_type := kwords[i, 2]]
}
# cbind(tst, dat[, study_type])

# dat[st == 5, .N] / nrow(dat)

kwords <- cbind(c("field", "direct", "paleo", "remote", "automated", "other"),
                c("field", "field", "paleo", "remote", "automated", "other"))
cal[, study_type := tolower(study_type)]
# tst <- cal[, study_type]
for(i in 1:nrow(kwords)) {
  cal[like(study_type, kwords[i, 1]), study_type := kwords[i, 2]]
}
# cbind(tst, cal[, study_type])

# unique(dat$DOI)
# a <- unique(dato$DOI)
# b <- unique(dat$DOI)
# notin <- sort(a)[!sort(a) %in% sort(b)]
# dato[DOI %in% notin, 1:5, with = FALSE]
# dat[observer == "Ahmed" & journal == "Oecologia"]
# unique(cal$DOI) %in% unique(dat$DOI)
# match(cal$DOI, dat$DOI)
# match(dat$DOI, cal$DOI)
# dat[dat$DOI %in% cal$DOI, DOI]
# dat[cal$DOI %in% dat$DOI, ]
# cal[DOI == dat[cal$DOI %in% dat$DOI, DOI][11]]
# dat[DOI == dat[cal$DOI %in% dat$DOI, DOI][7]]
# dat[observer == "Treuer", unique(DOI)]  # 19

```

## Data prep
### Merge full and calibration datasets
```{r, message=FALSE, warning=FALSE, results='hide'}
# dat2 <- copy(dat[, .(st, DOI, study_type, plot_res, n_sites, act_ext, eff_ext,
#                      samp_dur, t_btwn_samp, act_dur, eff_dur)])
# rnms <- c("st", "DOI", "study_type", "feature", "observer", "plot_res", 
#           "n_sites", "act_ext", "eff_ext", "samp_dur", "t_btwn_samp",
#           "act_dur", "eff_dur")
dat2 <- copy(dat[, .(DOI, study_type, plot_res, n_sites, act_ext, eff_ext,
                     samp_dur, t_btwn_samp, act_dur, eff_dur)])
# selind <- 3:ncol(dat2)
selind <- 4:ncol(dat2)
for(j in names(dat2)[selind]) set(dat2, j=j, value = as.numeric(dat2[[j]]))
rnms <- c("DOI", "study_type", "feature", "observer", "plot_res",
          "n_sites", "act_ext", "eff_ext", "samp_dur", "t_btwn_samp",
          "act_dur", "eff_dur")
calr <- cal[, rnms, with = FALSE]
selind <- c(3, 5:length(rnms))
for(j in names(calr)[selind]) set(calr, j=j, value = as.numeric(calr[[j]]))

# str(calr)
# select down to columns of interest
# st <- calr$study_type[which(calr$feature != 99)]
calr_type <- calr[feature != 99, { 
  list(unique(study_type[!is.na(study_type)]))
}, by = .(DOI, feature)][, list("study_type" = unique(V1)), by = DOI]

# calr[DOI == "10.1007/s00442-005-0338-3" & feature == 2, 
#      mean(eff_dur, na.rm = T)]
calr <- calr[feature != 99, {
  lapply(.SD, function(x) mean(x, na.rm = TRUE))
},  by = .(DOI, feature), .SDcols = rnms[c(5:ncol(calr))]]
length(unique(calr$DOI)) + length(unique(dat$DOI))  # 134 papers providing dat
# unique(calr$DOI)

# calr_type <- data.table(cbind("DOI" = unique(calr$DOI), # calib field studies
#                               "study_type" = c(rep("field", 5), "paleo",
#                                                rep("field", 4)))) 

# combine full and calibration
# calr[, samp_dur := NA]
calr[, samp_dur := NA]
calr[, "feature" := NULL] #c("DOI", "feature") := NULL]
calr <- merge(calr, calr_type, by = "DOI")
setcolorder(calr, names(dat2))
datf <- rbind(cbind("dsrc" = 1, dat2), cbind("dsrc" = 2, calr))
datf[act_dur > eff_dur]
doisup <- c(unique(cal$DOI_data_source), unique(dat$DOI_data_source))
doisup <- doisup[!is.na(doisup) & doisup != "NA" & doisup != "-"]
doisup <- unlist(strsplit(doisup, ";"))
```

`r length(unique(datf$DOI))` papers  main analysis

`r length(unique(full)) + (57 - 19)` papers main analysis (adding in Tim's rejected number separately)

`r (length(unique(full)) + (57 - 19)) / 42918 * 100` % of all papers since 2004

`r length(unique(doisup))` additional papers or other publications tracked down

`r nrow(datf)` records (ecological observations)

<a href="#top">Back to top</a>

### Adjustments/fixes
1. One could argue that if `samp_dur != act_dur` and `t_btwn_samp == 0`, set `act_dur == samp_dur`, but in a small number of cases there is a good reason to have to `act_dur != samp_dur` (e.g. DOI 10.1890/08-0611.1 when farmers were asked about their seasonal management practices--time period of interview was clearly != to information being collected))
2. Prior to scaling the data in log10 space, we need to make one adjustment. Time between samples has many 0 values because many observations are simply one-offs.  Set these to an arbitrarily large value which will represent clear separation on axis from high frequency studies. A function is defined here for this purpose, and will be applied after bootstrapping is done. 

```{r, message=FALSE, warning=FALSE, results='hide'}
# apply fixes (mostly to account for observer omissions)
# 1. if samp_dur != act_dur and t_btwn_samp == 0, set 
#    act_dur == samp_dur
names(datf)
datf[t_btwn_samp == 0 & (act_dur != samp_dur), ]  
dat[t_btwn_samp == 0 & (act_dur != samp_dur), c(1, 3:4, 9:15), with = FALSE]  
# function to set t_btwn_samp to 365 * 100000 to indicate once-off studies 
# when t_btwn_samp == 0.  
dat[act_dur < 1 & t_btwn_samp > 1000, ]
dat[act_dur > 1 & t_btwn_samp == 0, c(1, 3:4, 9:15), with = FALSE]
```

<a href="#top">Back to top</a>

## Analyses
### Resampling with uncertainties

Defined by the per variable CV based on uncertainty between observers. Here we are just going to use apply the variability to each variable for every observation (previous incarnation used uncertainty on just the variables listed as uncertain, with recalculation of dependent variables (`act_dur`, `act_ext`) made after pertubation [see commit prior to second or third commit on 4 April for code]).

```{r, eval=FALSE}
# p <- c(0.64, 0.84, 0.53, 1.21)  # from calibration.Rmd
# p <- c(0.58, 0.82, 0.99, 1.24)  # from calibration.Rmd
##     plot_res   n_sites   act_ext   eff_ext t_btwn_samp  act_dur   eff_dur
## 1: 0.5769693 0.3821107 0.7196559 0.4049972     1.01134 1.253045 0.5284913
##    plot_res n_sites  act_ext  eff_ext t_btwn_samp  act_dur  eff_dur
##       0.494   0.457    0.749    0.499       0.475    1.093    0.338
##    plot_res n_sites  act_ext  eff_ext t_btwn_samp  act_dur  eff_dur
##       0.577   0.382     0.72    0.405       1.045    1.261    0.643
p <- c(0.58, 0.72, 0.41, 1.05, 1.26, 0.64)  # from calibration.Rmd

# Functions for bootstrapping with uncertainties
# sets unreplicated observations to arbitrarily large number
to_infin_byond <- function(tbtwn) ifelse(tbtwn == 0, 365 * 10000, tbtwn)

# bootstrapping function
resamp_func <- function(dat, vars, p, iter) {
  bout <- lapply(1:iter, function(x) {  # x <- 1
    if((x / 100) %in% 1:100) print(x)
    if(length(p) < length(var)) stop("p must be length of var", call. = FALSE)
    
    # dnew <- copy(datf[, .(plot_res, act_ext, eff_ext, t_btwn_samp, act_dur)])
    # dnew <- copy(datf[, vars, with = FALSE])
    dnew <- copy(dat[, vars, with = FALSE])
    # modifiers
    modvec <- sapply(p, function(y) {
      mv <- runif(nrow(dnew), min = 1 - y, max = 1 + y)
      mv
    })
    # plot(dnew[[j]] * modvec[, j], dnew[[j]])
    
    # perturb and set any values < 0 to very small amount
    for(j in 1:ncol(modvec)) set(dnew, j = j, value = dnew[[j]] * modvec[, j])
    dnew[plot_res <= 0, plot_res := 0.00000001]
    dnew[act_ext <= 0, act_ext := 0.00000001]
    dnew[eff_ext <= 0, eff_ext := 0.00000001]
    dnew[act_dur <= 0, act_dur := 0.00000001]
    dnew[eff_dur <= 0, eff_dur := 0.00000001]
    dnew[t_btwn_samp < 0, t_btwn_samp := 0.00000001]

    dnew[, t_btwn_samp := to_infin_byond(t_btwn_samp)]  # set high
    dnew
  })
  return(bout)
}

# Run bootstrap
vars <- c("plot_res", "act_ext", "eff_ext", "t_btwn_samp", "act_dur", "eff_dur")
set.seed(1)
bootperturb <- resamp_func(datf, vars = vars, p = p, iter = 1000) # 10% perturb
# bootperturb2 <- resamp_func(datf, p = p, iter = 1000)  # 10% perturb
# all(bootperturb[[200]]$plot_res == bootperturb2[[200]]$plot_res)
# rm(bootperturb2)
```

```{r, echo = FALSE, eval=FALSE}
# check
par(mfrow = c(2, 3))
for(i in sample(1:1000, 6)) {
  plot(datf[, eff_ext], datf[, eff_ext] - bootperturb[[i]][, eff_ext], 
       pch = 20)
}
dev.off()
```

<a href="#top">Back to top</a>

## Plots
### Scaling in log10 space
```{r, eval = FALSE}
i <- 0.0000001
j <- rep(0, 16)
for(k in 1:length(j)) {
  i <- i * 10
  j[k] <- i
}

# temporal scales
tdt <- data.table("scaleval" = j, "time" = j)
tdt[, tlog := log10(time)]

# labels for temporal axes
# return interval
tlab1 <- c(expression(paste(NULL<="second"^-1)), expression("minute"^-1),
           expression("hour"^-1), expression("day"^-1), 
           expression("week"^-1), expression("month"^-1),            
           expression("year"^-1), expression("decade"^-1),
           expression("century"^-1), 
           expression("millenium"^-1), "unreplicated")# expression(infinity))
taxis1 <- cbind.data.frame( 
  c(1 / (24 * 60 * 60), 1 / (24 * 60), 1 / 24, 1, 7, 30, 365, 365 * 10, 
    365 * 100, 365 * 1000, 365 * 10000))
taxis1 <- data.table(taxis1)
setnames(taxis1, names(taxis1), "days")
taxis1[, logdays := log10(days)]

tlab2 <- c(expression(NULL<="second"), "minute", "hour", "day", "week", "month",
           "year","decade", "century", "millenium", "10 KA")  
taxis2 <- cbind.data.frame(c(1 / (24 * 60 * 60), 1 / (24 * 60), 1 / 24, 1, 7, 
                             30, 365, 365 * 10, 365 * 100, 365 * 1000, 
                             365 * 10000))
taxis2 <- data.table(taxis2)
setnames(taxis2, names(taxis2), c("days"))
taxis2[, logdays := log10(days)]

# spatial
# plot(1:10, xlab = expression(paste("mm"^2)), xaxt = "n")
# axis(1, at = 1:8, labels = lab, las = 2)

# plot resolution
alab1 <- c(expression(paste(NULL<="0.01 cm"^2)), expression("0.1 cm"^2), 
           expression("1 cm"^2), expression("10 cm"^2),
           expression("100 cm"^2), expression("1000 cm"^2), 
           expression("1 m"^2), expression("10 m"^2),
           expression("100 m"^2), expression("1000 m"^2), 
           "1 ha", "10 ha", "100 ha", "1000 ha", 
           expression(paste(NULL>="10000 ha")))

aaxis1 <- cbind.data.frame(c(0.000001, 0.00001, 0.0001, 0.001, 0.01, 0.1,
                             1^2, 10, 10^2, 10^3, 10^4, 10^5, 10^6, 10^7, 10^8))
aaxis1 <- data.table(aaxis1)
setnames(aaxis1, names(aaxis1), "res")
aaxis1[, logres := log10(res)]

# effective extent
alab2 <- c(expression(paste(NULL<="0.01 m"^2)), expression(paste("0.1 m"^2)), 
           expression("1 m"^2), expression("10 m"^2), 
           expression("100 m"^2), expression("1000 m"^2), expression("1 ha"),
           expression("10 ha"), expression("100 ha"), expression("1000 ha"), 
           expression(paste(10^4 , " ha")), expression(paste(10^5 , " ha")), 
           expression(paste(10^6 , " ha")), expression(paste(10^7 , " ha")), 
           expression(paste(10^8 , " ha")), expression(paste(10^9 , " ha")), 
           expression(paste(10^10 , " ha")))

aaxis2 <- cbind.data.frame(
  c(0.01 / 10000, 0.1 / 10000, 1 / 10000, 10 / 10000, 100 / 10000, 1000 / 10000,
    1, 10, 10^2, 10^3, 10^4, 10^5, 10^6, 10^7, 10^8, 10^9, 10^10))
aaxis2 <- data.table(aaxis2)
setnames(aaxis2, names(aaxis2), "area")
aaxis2[, logarea := log10(area)]
```
```{r, eval = FALSE, echo = FALSE}
save(aaxis1, aaxis2, taxis1, taxis2, alab1, alab2, tlab1, tlab2, 
     file = fp(p_dat, "derived/dimbreaks.rda"))
```

<a href="#top">Back to top</a>

### Figure 1 

Histograms for each of the four main dimensions.
```{r, eval=FALSE}
# length(which(datf$study_type %in% c("remote", "other"))) / nrow(datf)
datf[t_btwn_samp == 0, .N] / datf[, .N]  # 37% of samples are once-offs
datf[act_dur == samp_dur, .N] / nrow(datf)
datf[(act_dur == samp_dur) & t_btwn_samp != 0]
datf[(act_dur != samp_dur) & t_btwn_samp == 0]

hdat <- cbind("res" = log10(datf$plot_res), "aext" = log10(datf$act_ext),
              "ext" = log10(datf$eff_ext), 
              "int" = log10(to_infin_byond(datf$t_btwn_samp)), 
              "adur" = log10(datf$act_dur), "dur" = log10(datf$eff_dur))
# NA data checks
# hdat[is.na(rowSums(hdat)), ]
# dat[DOI %in% datf[which(is.na(rowSums(hdat))), DOI], ]
# dat[DOI %in% datf[which(is.na(hdat[, 6])), DOI]]
# dat[DOI %in% datf[which(is.na(hdat[, 3])), DOI]]
# calr[DOI %in% datf[which(is.na(hdat[, 3])), DOI] & is.na(eff_ext)]

hdat <- data.table(hdat)

# set bounds for histograms
# first set variable limits
vlr <- range(aaxis1$logres)  # for resolution
vle <- range(aaxis2$logarea)  # for extent

# then apply limits to main (unperturbed) set of variables
hdat[res < vlr[1], res := vlr[1]]  # set min res to 0.01 cm"^2 
hdat[res > vlr[2], res := vlr[2]]  # set max res to 10000 ha 
hdat[ext < vle[1], ext := vle[1]]  # set minimum extent -6 (0.01 m2) 
hdat[ext > vle[2], ext := vle[2]]  # set maximum extent 10 (10^10 ha)
# hdat[aext < vle[1], aext := vle[1]]  # set minimum extent -6 (0.01 m2) 
# hdat[aext > vle[2], aext := vle[2]]  # set maximum extent 10 (10^10 ha)


# histograms from bootstrap
brksl <- list(aaxis1$logres, aaxis2$logarea, taxis1$logdays, taxis2$logdays)  

# Assign bootstrapped values to bins, making necessary adjustments as needed 
vs <- c("res", "ext", "int", "dur")
hdat_bs <- lapply(1:length(bootperturb), function(x) {  # x <- 1
  print(x)
  DT <- bootperturb[[x]]
  hdb <- cbind("res" = log10(DT$plot_res), "ext" = log10(DT$eff_ext), 
               "int" = log10(to_infin_byond(DT$t_btwn_samp)), 
               "dur" = log10(DT$eff_dur))
  hdb <- data.table(hdb)
  hdb[res < vlr[1], res := vlr[1]]  # set min res to 0.01 cm"^2 
  hdb[res > vlr[2], res := vlr[2]]  # set max res to 10000 ha 
  # hdb[, lapply(.SD, range, na.rm = TRUE)]
  hdb[ext < vle[1], ext := vle[1]]  # set minimum extent -6 (0.01 m2)
  hdb[ext > vle[2], ext := vle[2]]  # set maximum extent 10 (10^10 ha)
  hists <- lapply(1:length(vs), function(y) {  # y <- 2
    #print(y)
    hdbv <- hdb[[vs[y]]]
    hbrks <- brksl[[y]]
    hdbv[hdbv < hbrks[1]] <- hbrks[1]
    hdbv[hdbv > hbrks[length(hbrks)]] <- hbrks[length(hbrks)]
    h <- hist(hdbv, breaks = hbrks, plot = FALSE)
    h$density <- h$counts / sum(h$counts) * 100
    h
  })
})

# convert density values for each dimension to data.tables
hdat_bsdt <- lapply(1:length(vs), function(x) { # x <- 1
  mat <- do.call(rbind, lapply(1:length(hdat_bs), function(y) {
    hdat_bs[[y]][[x]]$density
  }))
  DT <- data.table(mat)
})

### Bootstrapped histograms (now main figure)
qtf <- function(x) c(mean(x), quantile(x, probs = c(0.025, 0.975))) # quant f
xlabs <- c("Spatial resolution", "Spatial extent", "Sampling interval", 
           "Temporal duration")
# axes <- list("aax1" = aaxis1$logres, "aax2" = c(-6, aaxis2$logarea), 
#              "tax1" = taxis1$logdays, "tax2" = taxis2$logdays)
axes2 <- list("aax1" = aaxis1$logres, "aax2" = aaxis2$logarea, 
              "tax1" = c(taxis2$logdays[-c(10:11)],
                         mean(taxis2$logdays[c(10:11)])), 
              "tax2" = taxis2$logdays)
axlabs <- list(alab1, alab2, tlab1[-10], tlab2)
mga <- -0.34
sfigl <- -1.2
cxa <- 0.8
cxl <- 0.9
reds <- brewer.pal(9, name = "Reds")
blues <- brewer.pal(9, name = "Blues")
# plot(1:2, pch = 20, col = blues[c(4, 6)])
yl <- 35
cols <- c(reds[c(4, 6)], blues[c(4, 6)])

# calculate mean, 2.5th, 97.5th
hdatstat_bt <- lapply(1:length(hdat_bsdt), function(x) {
  h <- hdat_bsdt[[x]][, lapply(.SD, qtf)]
  hmu <- unlist(h[1, ])
  h2 <- unlist(h[2, ])
  h98 <- unlist(h[3, ])
  list("mu" = hmu, "p2" = h2, "p98" = h98)
})
names(hdatstat_bt) <- vs

# pdf("paper/figures/hists_bs.pdf", width = 5, height = 5)
png("paper/figures/hists_bs.png", width = 5, height = 5, res = 600, 
    units = "in")
par(mfrow = c(2, 2), mar = c(4.5, 1, 1, 1), oma = c(2, 3, 0, 0))

for(i in 1:length(hdatstat_bt)) {  # i <- 1
  hvals <- hdatstat_bt[[i]]
  axv <- brksl[[i]]
  mids <- sapply(2:length(axv), function(x) mean(c(axv[x], axv[x - 1])))

    # base plot
  yl <- ifelse(i %in% c(1, 2), 25, 40) 
  plot(c(axv[1], axv[length(axv)]), c(0, yl), ylab = "", las = 2, 
         xaxt = "n", xlab = xlabs[i], ylim = c(0, yl), main = "", 
         col = cols[i], cex.lab = cxl, pch = "", bty = "l", axes = FALSE)
  
  # create polygons to mimic histograms of varying width
  sapply(2:length(axv), function(x) {  # x <- 2
    px <- axv[c(x - 1, x)]
    pxs <- c(rep(px[1], 2), rep(px[2], 2), px[1])
    pys <- c(0, rep(hvals$mu[[x - 1]], 2), 0, 0)
    polygon(pxs, pys, col = cols[i])
  })
  axis(1, at = axes2[[i]], labels = axlabs[[i]], las = 2, 
       cex.axis = cxa, tcl = -0.2, mgp = c(2, 0.25, mga))
  axis(2, las = 2, cex.axis = cxa, tcl = -0.2, mgp = c(2, 0.25, mga))
  if(i %in% c(1, 3)) {
    mtext("Percent of observations", side = 2, line = 1.5, cex = 0.8)
  }
  mtext(LETTERS[i], side = 3, line = sfigl, cex = 0.8, adj = 0.05)

  # confidence intervals
  points(mids, hvals$p2, pch = "-", cex = 1, col = "grey40")
  points(mids, hvals$p98, pch = "-", cex = 1, col = "grey40")

}
dev.off()

# stats
# bootp_dt
# axisl <- list(aaxis1$logres, c(-6, aaxis2$logarea), taxis1$logdays,
#               taxis2$logdays)
# hdatstat <- lapply(1:4, function(x) {
#   h <- hist(hdat[[x]], breaks = brksl[[x]], plot = FALSE)
#   h$density <- h$counts / sum(h$counts) * 100
#   h
# })
```
```{r, echo = FALSE, eval = FALSE}
save(hdat, hdatstat_bt, file = fp(p_dat, "derived/hdatstat.rda"))
```

Statistics associated with Figure 1: percentage of observations falling within different scale ranges, and associated with different observational methods
```{r, echo = FALSE}
load(fp(p_dat, "derived/hdatstat.rda"))
load(fp(p_dat, "derived/dimbreaks.rda"))
```
```{r}
# res
# sum(hdatst[4:6])  # 53 % between 10cm^2 and 1 m^2
# sum(hdatst[1:6])  #  67% < 1m^2
# sum(hdatst[7:10]) # 24 % between 1 m^2 and 1 ha
# sum(hdatst[11:(l)]) # 9 % 1-10000 ha
# dat[plot_res >= 10000^2]
hdatst <- hdatstat_bt$res$mu
l <- length(hdatst)
hds <- c("10cm^2 - 1 m^2", "<1m^2", "1 m^2-1 ha", "1-10000 ha")
inds <- list(4:6, 1:6, 7:10, 11:l)
sapply(1:length(inds), function(x) {
  v <- round(sum(hdatst[inds[[x]]]), 1)
  names(v) <- hds[x]
  v
})

# length(which(hdat$res >= -3 & hdat$res <= 0)) / length(hdat$res)
# length(which(hdat$res <= 4)) / length(hdat$res)
# sum(hdatstat[[1]]$counts[4:6]) / sum(hdatstat[[1]]$counts)

# extent
# hdatst <- hdatstat[[2]]$density
hdatst <- hdatstat_bt$ext$mu
l <- length(hdatst)
# sum(hdatst[1:2])  # 36 % < 1 m^2
# sum(hdatst[1:6])  # 82% < 1 ha
# sum(hdatst[1:7])  # 86% < 10 ha
# sum(hdatst[1:8])  # 90 % < 100 ha
# sum(hdatst[1:9])  # 92 % < 1000 ha
# sum(hdatst[8:9])  # 6 % 10-1000 ha
# sum(hdatst[8:l])  # 14% > 10 ha
# sum(hdatst[10:l]) # 8.2 % covered an area > 1000 ha
# sum(hdatst[12:l]) # 5.1 % covered an area > 100,000 ha
# sum(hdatst[13:l]) # 3.9 % covered an area > 1,000,000 ha
hds <- c("<1 ha", "<10 ha", "<100 ha", "<1000ha", "10-1K ha", ">10ha", "1K-1MK",
         ">1Kha", ">100Kha", ">1Mha")
inds <- list(1:6, 1:7, 1:8, 1:9, 8:9, 8:l, 10:12, 10:l, 12:l, 13:l)
sapply(1:length(inds), function(x) {
  v <- round(sum(hdatst[inds[[x]]]), 1)
  names(v) <- hds[x]
  v
})

# interval
# hdatst <- hdatstat[[3]]$density
hdatst <- hdatstat_bt$int$mu
l <- length(hdatst)
# sum(hdatst[l])  # 36% are once-offs
# sum(hdatst[1:3])  # 15 % < daily
# sum(hdatst[1:4])  # 25 % < weekly
# sum(hdatst[4:5])  # 21% daily up to monthly
# sum(hdatst[6])  # 19% monthly up to yearly
# sum(hdatst[7:8]) # 8.2 % yearly to decadal
hds <- c("norep", "<daily", "<weekly",  "day-mo", "mo-yr", "yr-dec")
inds <- list(l, 1:3, 1:4, 4:5, 6, 7:8)
sapply(1:length(inds), function(x) {
  v <- round(sum(hdatst[inds[[x]]]), 1)
  names(v) <- hds[x]
  v
})

# duration
# hdatst <- hdatstat[[4]]$density
hdatst <- hdatstat_bt$dur$mu
l <- length(hdatst)
# sum(hdatst[1:3])  # 71 % up to 1 day
# sum(hdatst[4:5])  # 17% covered between 1 day and 1 month
# sum(hdatst[6])  # 7.3% covered between 1 month and 1 year
# # sum(hdatstat[[4]]$density[4:6])  # 26% covered between 1 day and 1 year
# sum(hdatst[7:10]) # 4.5% year to century
hds <- c("<=daily", "day-mo", "mo-yr", "yr-dec", "yr-cent")
inds <- list(1:3, 4:5, 6, 7, 8:10)
sapply(1:length(inds), function(x) {
  v <- round(sum(hdatst[inds[[x]]]), 1)
  names(v) <- hds[x]
  v
})


# datf[, plot(log10(n_sites), log10(plot_res))]
# dat[like(study_type, "field"), plot(log10(n_sites), log10(plot_res))]
# datf[like(study_type, "field"), plot(log10(plot_res), log10(act_ext), 
#                                      ylim = c(-10, 10))]
# datf[like(study_type, "remote"), 
#      points(log10(plot_res), log10(act_ext), pch = 20, col = "red")]
# datf[like(study_type, "automated"), 
#      points(log10(plot_res), log10(act_ext), pch = 20, col = "blue")]
# lines(aaxis1$logres, aaxis2$logarea[-c(16:17)] )
# datf[, .N / nrow(datf) * 100, by = st]

# datf[, plot(log10(t_btwn_samp), log10(act_dur), pch = 20)]
# datf[which(hdat[[3]] < -1.380211), 
#      points(log10(t_btwn_samp), log10(act_dur), pch = 20, col = "blue")]
# datf[like(study_type, "automated"), 
#      points(log10(t_btwn_samp), log10(act_dur), pch = 20, col = "red")]

# types of observations
# unique(datf$study_type)
datf[, round(.N / nrow(datf) * 100, 2), by = study_type]
round(datf[study_type == "automated" & log10(t_btwn_samp) < -1.380211, .N] / 
        hdat[int < -1.380211, .N] * 100, 2)
```

<a href="#top">Back to top</a>

### Figure 2 
```{r, eval = FALSE}
bootp_dt <- rbindlist(bootperturb)

# choose which dataset to use
# kdat <- datf  # ordinary
kdat <- bootp_dt  # bootstrap
resdat3 <- data.table(x = log10(to_infin_byond(kdat$t_btwn_samp)), 
                      y = log10(kdat$plot_res))
resdat3 <- resdat3[round(x, 8) != 6.56229286]  # remove one-time observations
resdat3[, n := 1]  # set up count column
# resdat3$n <- rep(1, nrow(resdat3))
rx <- range(resdat3$x)
rx <- round(rx / 0.5) * 0.5  # round to nearest 0.5
rx[1] <- -5  # set min interval to less than second
#rx[2] <- 6.56
rx[2] <- 5.57  # set max interval to millenium
resdat3[x < rx[1], x := rx[1]]
resdat3[x > rx[2], x := rx[2]]
ry <- range(aaxis1$logres)
ry[1] <- -5  # set limits to 0.01 cm^2
resdat3[y < ry[1], y := ry[1]]
resdat3[y > ry[2], y := ry[2]]
coordinates(resdat3) <- ~x + y

# extent
extdat3 <- data.table(x = log10(kdat$eff_dur),#log10(kdat$act_dur), 
                      y = log10(kdat$eff_ext))#log10(kdat$act_ext))
extdat3 <- extdat3[!is.na(x) & !is.na(y)] # cut out null values
# extdat3$n <- rep(1, nrow(extdat3))
extdat3[, n := 1]  # set up count column
ex <- range(taxis2$logdays)
ex <- c(floor(ex[1]), 5.56) ### should this be 5.57?  #ceiling(ex[2]))
extdat3[x < ex[1], x := ex[1]]
extdat3[x > ex[2], x := ex[2]]
ey <- range(aaxis2$logarea)
extdat3[y < ey[1], y := ey[1]]
extdat3[y > ey[2], y := ey[2]]
coordinates(extdat3) <- ~x + y

# temporal framing
tempdat <- data.table(x = log10(to_infin_byond(kdat$t_btwn_samp)), 
                      y = log10(kdat$eff_dur))#kdat$act_dur))
# tt <- data.table(x = log10(kdat$t_btwn_samp), y = log10(kdat$eff_dur))
# range(tt$x - tempdat$x)
tempdat <- tempdat[!is.na(x) & !is.na(y)] # cut out null values
tempdat <- tempdat[round(x, 8) != 6.56229286, ]  # remove one-time observations
# tempdat$n <- rep(1, nrow(tempdat))
tempdat[, n := 1]
tx <- range(tempdat$x)
tx <- round(rx / 0.5) * 0.5  # round to nearest 0.5
#tx[2] <- 6.56
tx[2] <- 5.57
tempdat[x < rx[1], x := rx[1]]
tempdat[x > rx[2], x := rx[2]]
ty <- range(taxis2$logdays)
ty <- c(floor(ty[1]), 5.56)#ceiling(ex[2]))
tempdat[y < ty[1], y := ty[1]]
tempdat[y > ty[2], y := ty[2]]
coordinates(tempdat) <- ~x + y

# spatial framing
spatdat <- data.table(x = log10(kdat$plot_res), y = log10(kdat$eff_ext))#act_ext))
spatdat <- spatdat[!is.na(x) & !is.na(y)] # cut out null values
# spatdat$n <- rep(1, nrow(spatdat))
spatdat[, n := 1]
sx <- range(aaxis1$logres)
sx[1] <- -5  # set limits to 0.01 cm^2
spatdat[x < sx[1], y := sx[1]]
spatdat[x > sx[2], y := sx[2]]
sy <- range(aaxis2$logarea)
spatdat[y < sy[1], y := sy[1]]
spatdat[y > sy[2], y := sy[2]]
coordinates(spatdat) <- ~x + y

# densities
# resolution
rres3 <- kdensity(rx[1], rx[2], ry[1], ry[2], 0.1, resdat3, 1)
rres3 <- (rres3 / cellStats(rres3, sum)) * 100

# extent
rext3 <- kdensity(ex[1], ex[2], ey[1], ey[2], 0.1, extdat3, 1)
rext3 <- (rext3 / cellStats(rext3, sum)) * 100

# temporal
tres <- kdensity(tx[1], tx[2], ty[1], ty[2], 0.1, tempdat, 1)
tres <- (tres / cellStats(tres, sum)) * 100
# spatial
sres <- kdensity(sx[1], sx[2], sy[1], sy[2], 0.1, spatdat, 1)
sres <- (sres / cellStats(sres, sum)) * 100

brkfun <- function(ext, ival, n) {
  rng <- range(ext[is.finite(ext)])
  bwidth <- (rng[2] - rng[1]) / n
  brks <- seq(rng[1], rng[2], bwidth)
  brklabs <- seq(0, round(rng[2], 2), ival)
  list("brks" = brks, "labs" = brklabs)
}

# cxa = 0.7
ll <- 3.5
cxa = 0.8
stps <- sapply(c(3, 4), function(x) which(kdat$st == x))  # IDs rs/auto/paleo
pchs <- list(1, "+")
cexs <- c(0.4, 0.6)
cuts <- 40
bump <- 3
cols <- inferno(cuts + bump)[-c(2:(bump + 1))]
# plot(1:cuts, col = cols, pch = 20)

# pdf("paper/figures/res_v_extent.pdf", width = 7, height = 2.5)
pdf("paper/figures/kde45.pdf", width = 7, height = 6)
par(mfrow = c(2, 2), mar = c(6, 6, 0.3, 3), oma = c(0, 0, 0.5, 1), 
    mgp = c(2, 0.25, 0))
# resolutions
# image(rres3, col = rev(terrain.colors(30)), breaks = brks,
image(rres3, axes = FALSE, xlab = "", ylab = "", #rev(terrain.colors(cuts))) 
      col = cols)
axis(1, at = taxis1$logdays[-c(5)], labels = tlab1[-c(5)], 
     las = 2, tcl = -0.2, cex.axis = cxa)
axis(2, at = aaxis1$logres[-1], labels = alab1[-1], las = 2, tcl = -0.2, 
     cex.axis = cxa)
mtext("Interval", 1, cex = cxa, line = ll)
mtext("Resolution", 2, cex = cxa, line = ll)
brks <- brkfun(rres3, 0.01, cuts)  # breaks
aargs <- list(mgp = c(3, 0.25, 0), at = brks$labs, labels = brks$labs, 
              cex.axis = cxa, tcl = -0.1)
plot(rres3, legend.only = TRUE, axis.args = aargs, legend.width = 1.25,
     #legend.shrink = 0.9, breaks = brks, col = rev(terrain.colors(30)))
     legend.shrink = 0.9, col = cols)#inferno(cuts))
     #col = rev(terrain.colors(cuts)))
mtext("A", side = 3, line = -1, cex = 0.8, adj = 0.05, col = "white")
# for(i in 1:length(stps)) {
#   points(resdat3$x[stps[[i]]], resdat3$y[stps[[i]]], pch=pchs[[i]], cex=cexs[i])
# }

# extents
# image(rext3, col = rev(terrain.colors(30)), axes = FALSE, xlab = "", 
image(rext3, axes = FALSE, xlab = "", ylab = "", col = cols)
axis(1, at = taxis2$logdays[-5], labels = tlab2[-5], las = 2,
     cex.axis = cxa, tcl = -0.2)
axis(2, at = aaxis2$logarea, labels = alab2, las = 2, cex.axis = cxa,
     tcl = -0.2)
brks <- brkfun(rext3, 0.01, cuts)  # breaks
aargs <- list(mgp = c(3, 0.25, 0), at = brks$labs, labels = brks$labs, 
              cex.axis = cxa, tcl = -0.1)
mtext("Duration", 1, cex = cxa, line = ll)
mtext("Extent", 2, cex = cxa, line = ll)
plot(rext3, legend.only = TRUE, axis.args = aargs, legend.width = 1.25,
     legend.shrink = 0.9, col = cols)
mtext("B", side = 3, line = -1, cex = 0.8, adj = 0.05, col = "white")

# spatial
image(sres, col = cols, axes = FALSE, xlab = "", ylab = "") 
axis(1, at = aaxis1$logres, labels = alab1, las = 2, tcl = -0.2, cex.axis = cxa)
axis(2, at = aaxis2$logarea, labels = alab2, las = 2, tcl = -0.2, 
     cex.axis = cxa)
mtext("Resolution", 1, cex = cxa, line = ll)
mtext("Extent", 2, cex = cxa, line = ll)
brks <- brkfun(sres, 0.01, cuts)  # breaks
aargs <- list(mgp = c(3, 0.25, 0), at = brks$labs, labels = brks$labs, 
              cex.axis = cxa, tcl = -0.1)
plot(sres, legend.only = TRUE, axis.args = aargs, legend.width = 1.25,
     legend.shrink = 0.9, col = cols)
mtext("C", side = 3, line = -1, cex = 0.8, adj = 0.05, col = "white")
# for(i in 1:length(stps)) {
#   points(spatdat$x[stps[[i]]], spatdat$y[stps[[i]]], pch=pchs[[i]], cex=cexs[i])
# }

# temporal
image(tres, axes = FALSE, xlab = "", ylab = "", col = cols)
axis(1, at = taxis1$logdays[-c(5)], labels = tlab1[-c(5)], 
     las = 2, tcl = -0.2, cex.axis = cxa)
axis(2, at = taxis2$logdays[-5], labels = tlab2[-5], las = 2,
     cex.axis = cxa, tcl = -0.2)
mtext("Interval", 1, cex = cxa, line = ll)
mtext("Duration", 2, cex = cxa, line = ll)
# brks <- brkfun(tres, 0.02, 100)  # breaks
brks <- brkfun(tres, 0.01, cuts)  # breaks
aargs <- list(mgp = c(3, 0.25, 0), at = brks$labs, labels = brks$labs, 
              cex.axis = cxa, tcl = -0.1)
plot(tres, legend.only = TRUE, axis.args = aargs, legend.width = 1.25,
     legend.shrink = 0.9, col = cols)
mtext("D", side = 3, line = -1, cex = 0.8, adj = 0.05, col = "white")
dev.off()

# image(rres3, col = rev(terrain.colors(100)))
# image(rext3, col = rev(terrain.colors(100)))
# datf[, points(log10(eff_dur), log10(eff_ext), pch = 20, cex = 0.5)]
# datf[like(study_type, "remote"), {
#   points(log10(eff_dur), log10(eff_ext), pch = 20, cex = 0.5, col = "blue")
# }]
# points(resdat3$x, resdat3$y, pch = 20, cex = 0.5)
# image(sres, col = rev(terrain.colors(100)))
# datf[, points(log10(plot_res), log10(eff_ext), pch = 20, cex = 0.5)]
# datf[like(study_type, "remote"), {
#   points(log10(plot_res), log10(eff_ext), pch = 20, cex = 0.5, col = "blue")
#   abline(lm(log10(eff_ext) ~ log10(plot_res)))
# }]
# datf[like(study_type, "field"), {
#   points(log10(plot_res), log10(eff_ext), pch = 20, cex = 0.5, col = "red")
#   abline(lm(log10(eff_ext) ~ log10(plot_res)))
# }]
# points(spatdat$x, spatdat$y, pch = 20, cex = 0.5)
```

<a href="#top">Back to top</a>

### Figure 3
```{r, eval=FALSE}
# actual versus effective duration
kdat <- bootp_dt  # bootstrap
# tempdat2 <- data.table(x = log10(kdat$act_dur), y = log10(kdat$eff_dur))
tempdat2 <- data.table(x = log10(kdat$eff_dur), y = log10(kdat$act_dur))
dinds <- kdat[, which(t_btwn_samp == 365 * 10000)]  # drop unreplicated obs
tempdat2 <- tempdat2[-dinds, ]
tempdat2 <- tempdat2[!is.na(x) & !is.na(y)] # cut out null values
tempdat2[, n := 1]  # set up count column
tx <- range(taxis2$logdays)
tx <- c(floor(ex[1]), 5.56) ### should this be 5.57?  #ceiling(ex[2]))
tempdat2[x < tx[1], x := tx[1]]
tempdat2[x > tx[2], x := tx[2]]
ty <- range(taxis2$logdays)
ty <- c(floor(ex[1]), 5.56) ### should this be 5.57?  #ceiling(ex[2]))
tempdat2[y < tx[1], y := tx[1]]
tempdat2[y > tx[2], y := tx[2]]
coordinates(tempdat2) <- ~x + y

# extent
# extdat4 <- data.table(x = log10(kdat$act_ext), y = log10(kdat$eff_ext))
extdat4 <- data.table(x = log10(kdat$eff_ext), y = log10(kdat$act_ext))
extdat4 <- extdat4[!is.na(x) & !is.na(y)] # cut out null values
extdat4[, n := 1]  # set up count column
ex <- range(aaxis2$logarea)
extdat4[x < ex[1], x := ex[1]]
extdat4[x > ex[2], x := ex[2]]
ey <- range(aaxis2$logarea)
extdat4[y < ey[1], y := ey[1]]
extdat4[y > ey[2], y := ey[2]]
coordinates(extdat4) <- ~x + y

# densities
# temporal 
tres2 <- kdensity(tx[1], tx[2], ty[1], ty[2], 0.1, tempdat2, 1)
tres2 <- (tres2 / cellStats(tres2, sum)) * 100

# extent
rext4 <- kdensity(ex[1], ex[2], ey[1], ey[2], 0.1, extdat4, 1)
rext4 <- (rext4 / cellStats(rext4, sum)) * 100

# cxa = 0.7
ll <- 3.5
cxa = 0.7
stps <- sapply(c(3, 4), function(x) which(kdat$st == x))  # IDs rs/auto/paleo
pchs <- list(1, "+")
cexs <- c(0.4, 0.6)
cuts <- 40
bump <- 3
cols <- inferno(cuts + bump)[-c(2:(bump + 1))]
# plot(1:cuts, col = cols, pch = 20)

# par(mfrow = c(1, 2))
# datf[, {
#   # sel = which()
#   eext = log10(eff_ext)
#   aext = log10(act_ext)
#   edur = log10(eff_dur[t_btwn_samp != 365 * 10000])
#   adur = log10(act_dur[t_btwn_samp != 365 * 10000])
#   plot(aext, eext - aext)
#   plot(adur, edur - adur)
#   length(edur)
# }]

# Difference in effective versus actual extent/duration against actual ext/dur
hdat_bs2 <- lapply(1:length(bootperturb), function(x) {  # x <- 1
  print(x)
  DT <- bootperturb[[x]]
  edb <- DT[, lapply(.SD, log10), .SDcol = c("eff_ext", "act_ext")]
  # edb <- edb[eff_ext >= act_ext, ]  # drop eff_dur < act
  setnames(edb, c("eff_ext", "act_ext"), c("eext", "aext"))
  edb[, dif := eext - aext]

  tdb <- DT[t_btwn_samp != 365 * 10000, lapply(.SD, log10), 
            .SDcol = c("eff_dur", "act_dur")]
  # tdb <- tdb[eff_dur >= act_dur, ]  # drop eff_dur < act
  setnames(tdb, c("eff_dur", "act_dur"), c("edur", "adur"))
  # print(paste(edb[, .N], tdb[, .N]))
  tdb[, dif := edur - adur]
  return(list("edb" = edb, "tdb" = tdb, #"elm" = elm, "tlm" = tlm, 
              "Ns" = c(edb[, .N], tdb[, .N])))
})

# check number of drops due to eff < act
# hdat_bs2_ns <- do.call(rbind, lapply(hdat_bs2, function(x) x$Ns))
# summary(bootperturb[[1]][, .N] - hdat_bs2_ns[, 1])
# summary(bootperturb[[1]][t_btwn_samp != 365 * 10000, .N] - hdat_bs2_ns[, 2])

# extract regression coefficients
# hdat_bs2_elm <- do.call(rbind, lapply(hdat_bs2, function(x) coef(x$elm)[, 1]))
# hdat_bs2_tlm <- do.call(rbind, lapply(hdat_bs2, function(x) coef(x$tlm)[, 1]))

# calculate box statistics
# Extent differences
eint <- aaxis2$logarea
edif_int <- rbindlist(lapply(hdat_bs2, function(y) {  # y <- hdat_bs2[[2]]
  dd <- copy(y$edb[!is.na(aext)])
  dd[aext < ex[1], aext := ex[1]]  # set to min
  # dd[eext < ex[1], eext := ex[1]]  # set to min
  dd[, int := findInterval(aext, eint, rightmost.closed = TRUE)]
  # dd[, int := findInterval(eext, eint, rightmost.closed = TRUE)]
  # dd[, list("dif" = mean(dif, na.rm = TRUE), "N" = .N), by = int][order(int)]
}))
ints <- sort(edif_int[, unique(int)])
ebox <- lapply(ints, function(x) edif_int[int == x, lmisc::box_stats(dif)])
pctobs <- sapply(ints, function(x) edif_int[int == x, .N])
ewgt <- pctobs / sum(pctobs)
# round(cumsum(ewgt), 2)
ewgtint <- c(0, 1, 5, 10, 15, 20, 25) / 100  # weight intervals
ewgttxt <- c("  0", "  1", "  5", seq(10, 25, 5))
ecols <- findInterval(ewgt, ewgtint)  # corresponding color index

# temporal differences
dint <- taxis2$logdays[-5]
tx <- range(dint)
ddif_int <- rbindlist(lapply(hdat_bs2, function(y) {  # y <- hdat_bs2[[2]]
  dd <- copy(y$tdb[!is.na(adur)])
  dd[adur < tx[1], adur := tx[1]]  # set to min
  dd[adur > tx[2], adur := tx[2]]  # set to max
  dd[, int := findInterval(adur, dint, rightmost.closed = TRUE)]  # include high
}))
dints <- sort(ddif_int[, unique(int)])
tbox <- lapply(dints, function(x) ddif_int[int == x, lmisc::box_stats(dif)])
pctobs <- sapply(dints, function(x) ddif_int[int == x, .N])
dwgt <- pctobs / sum(pctobs)
# round(cumsum(dwgt), 2)

dwgtint <- c(0, 1, 5, 10, 15, 20, 25, 30) / 100  # weight intervals
dwgttxt <- c("  0", "  1", "  5", seq(10, 30, 5))
dcols <- findInterval(dwgt, dwgtint)  

# y grid line function
ylines <- function(yr, by, lty = 1, col = "grey") {
  abline(h = seq.int(from = yr[1], to = yr[2], by = 2), lty = lty, col = col)
}

# pdf("paper/figures/act_v_eff_diff.pdf", width = 7, height = 4)
png("paper/figures/act_v_eff_diff.png", width = 7, height = 4, res = 600, 
    units = "in")
par(mfrow = c(1, 2), mar = c(4, 0, 1, 0.75), oma = c(0, 3, 0, 0))
bx <- rowMeans(cbind(eint[-length(eint)], eint[-1]))
# cols <- sapply(ealphs, function(x) rgb(0, 0.1, 1, alpha = x))
cols <- brewer.pal(length(ewgtint) - 1, "Blues")
plot(ex, c(0, 14), pch = "", xlab = "", ylab = "", xaxt = "n", yaxt = "n",
     mgp = c(3, 0.5, 0), tcl = -0.2, xaxs = "i")
polygon(c(ex, rev(ex), ex[1]), c(-1, -1, 15, 15, -1), col = "grey90")
ylines(yr = c(0, 14), by = 2, col = "grey95")
for(i in 1:length(ebox)) {
  boxplot_v(bx[i], y = ebox[[i]], n = 1.75, whiskhgt = 1.5, 
            bfill = cols[ecols][i])
  text(bx[i], ebox[[i]][5] + 0.15, round(ewgt[i] * 100, 1), cex = 0.3)
} 
axis(1, at = eint, labels = alab2, las = 2, tcl = -0.2, cex.axis = cxa, 
     mgp = c(3, 0.5, 0))
axis(2, at = seq(0, 14, 2), las = 2, tcl = -0.2, cex.axis = cxa, 
     mgp = c(3, 0.5, 0))
mtext("Actual Extent", side = 1, line = 3, cex = 0.9)
mtext("Difference between Extent/Duration", side = 2, line = 2.2, cex = 0.9)
mtext("and Actual Extent/Duration (Decades)", side = 2, line = 1.4, cex = 0.9)
lrng <- c(mean(ex), max(ex) - 0.05 * max(ex))
xs <- seq(lrng[1], lrng[2], by = (diff(lrng)) / (length(cols)))
rect_shade(xs, y = c(11.5, 12.5), fillcol = cols, linecol = "grey30")
text(xs, rep(10.5, length(xs)), ewgttxt, srt = 90, col = "grey30")
text(mean(xs), 13, "% of observations", col = "grey30")
mtext("A", side = 3, line = -1, cex = 1, adj = 0.05)

# duration
bx <- rowMeans(cbind(dint[-length(dint)], dint[-1]))
bd <- abs(dint[-length(dint)] - dint[-1])
cols <- brewer.pal(length(dwgtint) - 1, "Reds")
plot(tx, c(0, 14), pch = "", xlab = "", ylab = "", xaxt = "n", xaxs = "i",
     mgp = c(3, 0.5, 0), tcl = -0.2, yaxt = "n")
polygon(c(tx, rev(tx), tx[1]), c(-1, -1, 15, 15, -1), col = "grey90")
ylines(yr = c(0, 14), by = 2, col = "grey95")
for(i in 1:length(tbox)) {
  boxplot_v(bx[i], y = tbox[[i]], n = 0.9, inhgt = bd[i] * 10, whiskhgt = 0.75, 
            bfill = cols[dcols][i])
  text(bx[i], tbox[[i]][5] + 0.15, round(dwgt[i] * 100, 1), cex = 0.3)

} 
axis(1, at = dint, labels = tlab2[-c(5)], las = 2, tcl = -0.2, cex.axis = cxa, 
     mgp = c(3, 0.5, 0))
axis(2, at = seq(0, 14, 2), labels = rep("", 8), las = 2, tcl = -0.2, 
     cex.axis = cxa, mgp = c(3, 0.5, 0))
mtext("Actual Duration", side = 1, line = 3, cex = 0.9)
lrng <- c(mean(tx), max(tx) - 0.05 * max(tx))
xs <- seq(lrng[1], lrng[2], by = (diff(lrng)) / (length(cols)))
rect_shade(xs, y = c(11.5, 12.5), fillcol = cols, linecol = "grey30")
text(xs, rep(10.5, length(xs)), dwgttxt, srt = 90, col = "grey30")
text(mean(xs), 13, "% of observations", col = "grey30")
mtext("B", side = 3, line = -1, cex = 1, adj = 0.05)
dev.off()

# Kernel density version
pdf("paper/figures/act_v_eff.pdf", width = 7, height = 3)
par(mfrow = c(1, 2), mar = c(4.5, 4.5, 0.3, 3), oma = c(0, 0, 0.2, 1), 
    mgp = c(2, 0.25, 0))

mtl <- 3
# spatial extents
image(rext4, axes = FALSE, xlab = "", ylab = "", col = cols)
axis(1, at = aaxis2$logarea, labels = alab2, las = 2,
     cex.axis = cxa, tcl = -0.2)
axis(2, at = aaxis2$logarea, labels = alab2, las = 2, cex.axis = cxa,
     tcl = -0.2)
brks <- brkfun(rext4, 0.01, cuts)  # breaks
aargs <- list(mgp = c(3, 0.25, 0), at = brks$labs, labels = brks$labs, 
              cex.axis = cxa, tcl = -0.1)
mtext("Actual extent", 2, cex = cxa, line = mtl)
mtext("Effective extent", 1, cex = cxa, line = mtl)
plot(rext4, legend.only = TRUE, axis.args = aargs, legend.width = 1.25,
     legend.shrink = 0.9, col = cols)
mtext("A", side = 3, line = -1, cex = 0.8, adj = 0.05, col = "white")

# temporal extents
image(tres2, axes = FALSE, xlab = "", ylab = "", col = cols)
axis(1, at = taxis1$logdays[-c(5)], labels = tlab1[-c(5)], 
     las = 2, tcl = -0.2, cex.axis = cxa)
axis(2, at = taxis1$logdays[-c(5)], labels = tlab1[-c(5)], las = 2, tcl = -0.2, 
     cex.axis = cxa)
mtext("Actual duration", 2, cex = cxa, line = mtl)
mtext("Effective duration", 1, cex = cxa, line = mtl)
brks <- brkfun(tres2, 0.01, cuts)  # breaks
aargs <- list(mgp = c(3, 0.25, 0), at = brks$labs, labels = brks$labs, 
              cex.axis = cxa, tcl = -0.1)
plot(tres2, legend.only = TRUE, axis.args = aargs, legend.width = 1.25,
     legend.shrink = 0.9, col = cols)
     #col = rev(terrain.colors(cuts)))
mtext("B", side = 3, line = -1, cex = 0.8, adj = 0.05, col = "white")
dev.off()


```

## Supplemental Analyses

### Sensitivity
Just for the four main variables. Rules: 

+ If `plot_res` and/or `n_sites` uncertain, then `act_ext` must be. Recalculate using random draw within the uncertain variable 
+ If `t_btwn_samp` and/or `samp_dur` uncertain, do the same as above, but we have no variable for n repeats for calculating `act_dur`.

```{r}
svars <- c("plot_res", "n_sites", "act_ext", "eff_ext", "samp_dur",
           "t_btwn_samp", "act_dur", "eff_dur")
sens <- gsub("\\\\", ";", dat$sensitivity)
sens <- gsub(", ", ";", gsub("; ", ";", sens))
sens <- tolower(sens)
sensl <- strsplit(sens, ";")
# unique(unlist(sensl))

sens2 <- gsub("plo_|plot-", "plot_", sens)  # fix this one
sens2 <- gsub("[[:blank:]]", "_", sens2)  # replace spaces with _
sens2 <- gsub("-", "_", sens2)  # replace - with _

corrtab <- cbind(c("^res", "plo_res", "plot_resolution", "plot_size", 
                   "sample_area", "samples_area", "sampled area", "study_area",
                   "sampled_area",
                   "t_btwn_samples", "t_btwn_sample", "t_btwn_samp_samp", 
                   "t_twn_samp", "t_btw_samp", "time_between_sampling", 
                   "time_between_samples", 
                   "samp_duratiion", "sampling_duration", "sample_duration", 
                   "number_of_sites", 
                   "study_span", "sutdy_span", "study_dur", "study_soan",
                   "samp_study", ":", " ", ";_", 
                   "duration", "dure", 
                   "extent"), 
                 c(rep("plot_res", 4), rep("act_ext", 5), rep("t_btwn_samp", 7),                    rep("samp_dur", 3), "n_sites", rep("eff_dur", 4), 
                   "samp;study", ";", ";", ";", 
                   "dur", "dur", "ext"))

# replace
for(i in 1:nrow(corrtab)) {
  sens2 <- gsub(corrtab[i, 1], corrtab[i, 2], sens2)
}
# gsub("[a-z]( )[a-z]", "_", sens[[228]])
# sens[[228]]
# sens2[is.na(sens2)]
sens2[is.na(sens2)] <- "none"
sensl2 <- lapply(1:length(sens2), function(x) strsplit(sens2[x], ";")[[1]])
# unique(unlist(sensl))
unique(unlist(sensl2))


# create a sensitivity table
senst <- do.call(cbind.data.frame, lapply(svars, function(x) {
  v <- sapply(sensl2, function(y) ifelse(any(y == x), 1, 0))
  # ifelse(is.na(v), 0, v)
}))
colnames(senst) <- c("res", "n", "aext", "eext", "sampd", "tbtwn", "actdur", 
                     "effdur")

# apply fixes (mostly to account for observer omissions)
# 1. if samp_dur OR t_btwn_samp is uncertain, act_dur must also be. 
id <- which((senst$sampd == 1 | senst$tbtwn == 1) & senst$actdur == 0)
senst[id, "actdur"] <- 1

# 2. if act_dur is uncertain but t_btwn and samp_dur are not, treat
# both as uncertain
# although it is possible that just n_repeats is uncertain, but more likely 
# that observers didn't note this is sensitive. 
id <- which(senst$tbtwn == 0 & senst$sampd == 0 & senst$actdur == 1)
senst[id, c("sampd", "tbtwn")] <- 1

# 3. if plot_res or n_sites is uncertain, then act_ext must be
id <- which((senst$res == 1 | senst$n == 1) & senst$aext == 0)
senst[id, "aext"] <- 1

# percentage of records that are uncertain
round(colSums(senst) / nrow(senst) * 100, 1)

```

<a href="#top">Back to top</a>

### Study type by year

Calculate types of study by year, as total and percent
```{r, warning = FALSE, message = FALSE, fig.height=3}
# One Oecologia study listed as 2004 (by Endnote) was actually 2003 
# Set it to 2004
stypes <- copy(dat)  # new dataset out of dat (which retain year info)
stypes[study_year < 2004, study_year := 2004] 

# bring in study_type from calr set, grabbing year from original cal dt
calyr <- cal[DOI %in% calr[, unique(DOI)], unique(study_year), by = DOI]
calyr[, V1 := as.numeric(V1)]
calyr <- calyr[, list("study_year" = mean(V1, na.rm = TRUE)), by = DOI]
calyr <- merge(calyr, calr[, list(DOI, study_type)], by = "DOI")

# bind
stypes <- rbind(stypes[, list(DOI, study_year, study_type)], calyr)
na_effext <- datf[is.na(eff_ext), unique(DOI)]  # ID NA eff_ext values

# Estimate and plot study type by year
stypeyr <- stypes[, list(.N, study_type), by = study_year][, {
  list("tot" = N, "ct" = .N, "prop" = .N / N * 100)
}, by = .(study_type, study_year)][order(study_year)]
stypeyr <- stypeyr[, lapply(.SD, mean), .SDcols = c("tot", "ct", "prop"), 
                   by = .(study_type, study_year)]
```
```{r, eval = FALSE}
obs_types <- c("remote", "field", "auto")
obs_types2 <- c("Remote sensing", "Field", "Automatic")
cols <- c("orange", "green4", "blue4")
png(fp(p_fig, "stype_by_yr.png"), width = 7, height = 2.75, res = 300, 
    units = "in")
par(mfrow = c(1, 3), oma = c(0, 3, 0, 0), mar = c(4, 1, 2, 1))
for(i in 1:3) {  # i <- 1
  plot(c(2004, 2014), c(0, 100), xlab = "", ylab = "", pch = "")
  stypeyr[like(study_type, obs_types[i]), {
    points(study_year, prop, col = cols[i], pch = 20)
    yr = as.numeric(study_year)
    slm = lm(prop ~ yr, weights = tot)  # weighted regression
    slms = summary(slm)
    abline(slm, col = cols[i])
    cf = round(coef(slm)[1:2], 4)
    pf = round(coef(slms)[8], 2)
    rf = round(slms$adj.r.squared, 2)
    cfa = paste0(cf[2], "% year")
    text(x = 2006, y = 100, labels = substitute(x^-1, list(x = cfa)), 
         cex = 0.9)
    text(x = 2011.5, y = 100, cex = 0.9,
         labels = substitute(R^2~"="~x~"; p <"~y, list(x = rf, y = pf)))
    if(i == 1) mtext("percent", side = 2, line = 2.5, cex = 0.8)
    if(i == 2) mtext("year", side = 1, line = 2.5, cex = 0.8)
    mtext(obs_types2[i], side = 3, cex = 0.8)
  }]
}
dev.off()
```

Extrapolating percentage of studies using remote sensing to 2017
```{r}
preds <- stypeyr[like(study_type, "remote"), {
  slm = lm(prop ~ study_year, weights = tot)
  predict(slm, newdata = data.frame("study_year" = 2004:2017))
}]
names(preds) <- 2004:2017
```

Percent of remote sensing studies by year
```{r}
round(preds, 1)
```

Average annual percent of studies using remote sensing 2004-2014, and projected increases in this percent through 2017 (2004-2017) 
```{r}
# murs2014 <- stypeyr[study_type == "remote", round(mean(prop), 2)]
murs2014 <- stypeyr[, sum(ct[study_type == "remote"]) / sum(ct) * 100]
pctinc <- mean(preds) / mean(preds[1:(length(preds) - 3)])

pcttab <- cbind("2014" = murs2014, "pct increase" = (pctinc - 1) * 100,
                "2017" = murs2014 * pctinc)
round(pcttab, 1)
```

```{r}
types <- datf[, .N, by = study_type]
rs2017 <- mean(preds)  # avg predicted % using RS through 2004-2017
rs2014 <- mean(preds[1:(length(preds) - 3)])  # avg predicted % RS 2004-2014
# rs2017 / rs2014

# translate to N more RS studies would have been selected given those %ages
rsinc <- rs2017 / rs2014 * types[study_type == "remote", N] - 
                 types[study_type == "remote", N]  

remote <- datf[study_type == "remote", eff_ext]
notremote <- datf[study_type != "remote", eff_ext]
# a <- (length(remote) + rsinc) / (nrow(datf))
# b <- length(remote) / nrow(datf)
# a / b
# ((length(remote) + rsinc) / (length(notremote)) / length(b))

# (rsext * rs2017 + nonrsext * (1 - rs2017)) / 
#   (rsext * rs2014 + nonrsext * (1 - rs2014))

set.seed(1)
ext2017 <- unlist(lapply(1:1000, function(x) {  # x = 1
  ind <- sample(1:length(remote), size = round(rsinc))
  mean(c(remote, remote[ind], notremote), na.rm = TRUE)
}))

dec1417 <- log10(mean(ext2017, na.rm = TRUE) / 
                   datf[, mean(eff_ext, na.rm = TRUE)])
qtiles1417 <- log10(quantile(ext2017, c(0.025, 0.975), na.rm = TRUE) / 
                      datf[, mean(eff_ext, na.rm = TRUE)])

pctdiff1417 <- mean(ext2017, na.rm = TRUE) / datf[, mean(eff_ext, na.rm = TRUE)]
pctqtiles1417 <- quantile(ext2017, c(0.025, 0.975), na.rm = TRUE) / 
                      datf[, mean(eff_ext, na.rm = TRUE)]

cbind("decade" = round(dec1417, 4), "dec2.5" = round(qtiles1417[1], 4),
      "dec97.5" = round(qtiles1417[2], 4),
      "pctdiff" = round((pctdiff1417 - 1) * 100, 1),
      "pctdiff2.5" = round((pctqtiles1417[1] - 1) * 100, 1), 
      "pctdiff97.5" = round((pctqtiles1417[2] - 1) * 100, 1))
```

### Change in dimensions by year
```{r}
# Examines and plot trends in dimensions
doiyr <- rbind(unique(dat[, .(DOI, study_year)]), 
               unique(cal[, .(DOI, study_year)]))
doiyr <- doiyr[study_year %in% 2004:2014]
datf2 <- copy(datf)
datf2 <- merge(doiyr, datf2, by = "DOI")  # collect year of publication
# datf2[, N := .N, by = study_year]
```
```{r, eval = FALSE}
vars <- c("plot_res", "eff_ext", "t_btwn_samp", "eff_dur")
varlabs <- c("Resolution", "Extent", "Interval", "Duration")
labcols <- c(brewer.pal(8, "Reds")[c(7, 7)], brewer.pal(8, "Blues")[c(7, 7)])
png(fp(p_fig, "dim_by_yr.png"), width = 7, height = 6, res = 300, 
    units = "in")
par(mfrow = c(2, 2), mar = c(1, 3, 1, 1), oma = c(2, 3, 0, 0))
for(i in 1:length(varlabs)) {  # i <- 1
  if(i == 3) {
    DT <- copy(datf2[t_btwn_samp != 0, ])
  } else {
    DT <- copy(datf2)
  }
  DT[, list(mean(get(vars[i]), na.rm = TRUE), .N), by = study_year][, {
    dim = log10(V1)
    yr = as.numeric(study_year)
    if(i %in% 1:2) rng = c(0, 10)
    if(i %in% 3:4) rng = c(0, 5)
    if(i %in% 3:4) {
      xlabs = seq(2004, 2014, 2)
    } else {
      xlabs = rep("", 6)
    }
    plot(yr, dim, ylim = rng, xaxt = "n", pch = 20, col = labcols[i], 
         main = varlabs[i], mgp = c(2, 0.5, 0), tcl = -0.2, ylab = "")
    axis(1, at = seq(2004, 2014, 2), labels = xlabs, tcl = -0.2, 
         mgp = c(2, 0.5, 0))
    slm = lm(dim ~ yr, weights = N)
    slms = summary(slm)
    abline(slm, col = labcols[i])
    cf = round(coef(slm)[1:2], 3)
    pf = round(coef(slms)[8], 2)
    rf = round(slms$adj.r.squared, 2)
    text(x = 2006.25, y = rng[2] * 0.97, cex = 0.9,
         labels = substitute(x~year^-1, list(x = cf[2])))
    text(x = 2011.5, y = rng[2] * 0.97, cex = 0.9,
         labels = substitute(R^2~"="~x~"; p <"~y, list(x = rf, y = pf)))
    if(i == 1) mtext(expression("log10 m"^2), side = 2, line = 2)
    if(i == 2) mtext("log10 ha", side = 2, line = 2)
    if(i %in% 3:4) mtext("log10 days", side = 2, line = 2)
  }]
}
dev.off()
```

Percent increase in average extent by year
```{r}
preds <- datf2[, list(mean(eff_ext, na.rm = TRUE), .N), by = study_year][, {
  dim = log10(V1)
  yr = as.numeric(study_year)
  slm = lm(dim ~ yr, weights = N)
  predict(slm, newdata = data.frame("yr" = 2004:2017))
}]
# preds[14] - preds[13]

# projected percent increase in extent 2014-2017
pctinc <- mean(preds) / mean(preds[1:(length(preds) - 3)])
muext2014 <- datf2[, mean(eff_ext, na.rm = TRUE)]
pcttab <- cbind("2014" = log10(muext2014), "pct increase" = (pctinc - 1) * 100,
                "2017" = log10(muext2014 * pctinc), 
                "decade" = log10((pctinc * muext2014) / muext2014))
round(pcttab, 3)
```

### Varying kernel sizes
```{r,eval = FALSE}
# redo range variables
rx <- range(resdat3$x)
rx <- round(rx / 0.5) * 0.5  # round to nearest 0.5
rx[1] <- -5  # set min interval to less than second
rx[2] <- 5.57  # set max interval to millenium
ex <- range(taxis2$logdays)
ex <- c(floor(ex[1]), 5.56) ### should this be 5.57?  #ceiling(ex[2]))
ey <- range(aaxis2$logarea)

# density rasters
# vary the kernel size about them
kernsl <- lapply(c(0.4, 0.7, 1), function(x) {
  rr <- kdensity(rx[1], rx[2], ry[1], ry[2], 0.1, resdat3, x)
  rr <- (rr / cellStats(rr, sum)) * 100
  er <- kdensity(ex[1], ex[2], ey[1], ey[2], 0.1, extdat3, x)
  er <- (er / cellStats(er, sum)) * 100
  kerns <- list("res" = rr, "ext" = er)
})

# plot parameters
cxa <- 1.25
cuts <- 40
bump <- 3
cols <- inferno(cuts + bump)[-c(2:(bump + 1))]

# As separate plots
pdf("paper/figures/res_v_extent_ksize2.pdf", width = 8, height = 10)
par(mfrow = c(3, 2), oma = c(6, 3, 0, 0), mar = c(1, 5, 3, 4))
for(i in 1:3) {  # i <- 3
  x <- kernsl[[i]] # x <- kernsl[[1]]
  image(x$res, col = cols, axes = FALSE, xlab = "", ylab = "")
  if(i == 3) {
    axis(1, at = taxis1$logdays[-c(5, 9, 10)], labels = tlab1[-c(5, 9, 10)], 
         las = 2, tcl = -0.2, cex.axis = cxa)
  }
  axis(2, at = aaxis1$logres[-1], labels = alab1[-1], las = 2, tcl = -0.2, 
       cex.axis = cxa)  
  aargs <- list(mgp = c(3, 0.25, 0), #at = brklabs, labels = brklabs, 
                cex.axis = cxa, tcl = -0.1)
  plot(x$res, legend.only = TRUE, axis.args = aargs, legend.width = 1.25,
     legend.shrink = 0.9, col = cols)#breaks = brks, 
  
  image(x$ext, col = cols, axes = FALSE, xlab = "", ylab = "")
  if(i == 3) {
    axis(1, at = taxis2$logdays[-5], labels = tlab2[-5], las = 2,
         cex.axis = cxa, tcl = -0.2)
  }
  axis(2, at = aaxis2$logarea, labels = alab2, las = 2, cex.axis = cxa,
       tcl = -0.2)
  aargs <- list(mgp = c(3, 0.25, 0), #at = brklabs2, labels = brklabs2, 
                cex.axis = cxa, tcl = -0.1)
  plot(x$ext, legend.only = TRUE, axis.args = aargs, legend.width = 1.25,
       legend.shrink = 0.9, col = cols)#breaks = brks2, 
}
# # par(xpd = NA)
# for(j in c("in", "dev", "ndc", "nfc", "npc", "nic")) {
#   xx <- grconvertX(0.5, from = "user", to = j)
#   ii <- c(0.4, 0.6, 0.8, 1)
#   yy <- c(0.95, 0.75, 0.5, 0.25)
#   # yy <- c(-1, -5, -10, -20)
#   for(i in 1:length(ii)) {
#     y <- grconvertY(yy[i], from = "user", to = j)
#     text(x = xx, y = y, labels = paste("kernel =", ii[i]))
#     # par(xpd = NA)
#     # mtext(text = paste("kernel =", ii[i]), side = 3, line = yy[i])
#   }
# }
dev.off()

# plot(rrs, xlim = c(-5, 15), ylim = c(-5, 15), 
#      xlab = "Sample interval - log10(days)", 
#      ylab = "Plot resolution - log10(m^2)")
# plot(ers, box = FALSE, xlim = c(-5, 15), ylim = c(-5, 15), 
#      xlab = "Study duration - log10(days)", ylab = "Sampled area - log10(ha)")
# dev.off()

```

<a href="#top">Back to top</a>

